https://arxiv.org/pdf/2602.04116

Toward Effective Multimodal Graph Foundation Model:
A Divide-and-Conquer Based Approach
Sicheng Liu * 1 Xunkai Li * 1 Daohan Su 1 Ru Zhang 1 Hongchao Qin 1 Ronghua Li 1 Guoren Wang 1
Abstract
Graph Foundation Models (GFMs) have achieved
remarkable success in generalizing across diverse
domains. However, they mainly focus on TextAttributed Graphs (TAGs), leaving MultimodalAttributed Graphs (MAGs) largely untapped. Developing Multimodal Graph Foundation Models
(MGFMs) allows for leveraging the rich multimodal information in MAGs, and extends applicability to broader types of downstream tasks.
While recent MGFMs integrate diverse modality
information, our empirical investigation reveals
two fundamental limitations of existing MGFMs:
â¶ they fail to explicitly model modality interaction, essential for capturing intricate cross-modal
semantics beyond simple aggregation, and â· they
exhibit sub-optimal modality alignment, which
is critical for bridging the significant semantic
disparity between distinct modal spaces.
To address these challenges, we propose
PLANET (graPh topoLogy-aware modAlity
iNteraction and alignmEnT), a novel framework
employing a Divide-and-Conquer strategy to decouple modality interaction and alignment across
distinct granularities. At the embedding granularity, â¶ Embedding-wise Domain Gating
(EDG) performs local semantic enrichment by
adaptively infusing topology-aware cross-modal
context, achieving modality interaction. At the
node granularity, â· Node-wise Discretization
Retrieval (NDR) ensures global modality alignment by constructing a Discretized Semantic Representation Space (DSRS) to bridge modality
gaps. Extensive experiments demonstrate that
PLANET significantly outperforms state-of-theart baselines across diverse graph-centric and multimodal generative tasks.
*Equal contribution 1Department of XXX, University of
YYY, Location, Country. Correspondence to: Ronghua Li
<lironghuabit@126.com>.
Preprint. February 5, 2026.
1. Introduction
In recent years, GFMs (Xia et al., 2024; Xia & Huang,
2024) have emerged as a transformative paradigm in graph
representation learning, offering a unified encoding framework capable of generalizing across cross-domain datasets.
However, most existing GFMs are primarily designed for
TAGs (He et al., 2025a; Wang et al., 2024b) or focus on
learning unified graph structures (Yu et al., 2025; Sun et al.,
2025), failing to extend effectively to MAGs (Zhu et al.,
2025a; Yan et al., 2025). This limitation implies significant missed improvements. From a data perspective, MAGs
enriched with diverse modalities offer significantly richer
multimodal semantic information compared to TAGs. From
an application perspective, extending GFMs to MAGs significantly broadens the applicable downstream tasks such as
modality retrieval task (Qu et al., 2021) and graph generative task (Yoon et al., 2023; Fang et al., 2025). To address
this limitation, MGFMs (He et al., 2025b; Fang et al., 2025)
have been introduced to integrate these heterogeneous signals within graph structures, achieve domain and modality
alignment.
To validate the advantages of incorporating multimodal information and the superiority of MGFMs approaches, we
conducted an empirical study shown in Fig. 1(a)&(b). Our
findings yield two key conclusions: â¶ The use of multiple modalities consistently improve model performance. â·
MGFMs significantly outperform GFMs. UniGraph2 (He
et al., 2025b) consistently achieves state-of-the-art results,
demonstrating the need for specialized architectures for
MAGs. Please refer to Appendix B.1 for detailed implementation.
Although MGFMs such as UniGraph2 have demonstrated a
powerful capacity to learn from diverse multimodal graphs,
they are fundamentally limited by their approach to modality
interaction and alignment. Specifically, UniGraph2 does
not account for the significant semantic disparities between
the latent spaces of different encoders and, crucially, lacks
modality interaction, which is verified to be important in
learning robust feature representations (Zhu et al., 2025a).
Furthermore, UniGraph2 lacks explicit supervisory signals
for modality alignment, rendering the alignment process
inefficient and inadequately constrained.
1
arXiv:2602.04116v1 [cs.LG] 4 Feb 2026
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach Accuracy (%)
MRR
Finetuning Epoch Finetuning Epoch
(a) LP on Amazon-Sports (b) NC on Movies (c) NC on Goodreads-NC (d) NC on RedditS
Figure 1. Empirical study results. (a)&(b) Performance comparison across different modalities and architectures. (c)&(d) Stepwise
enhancement results.
To empirically substantiate these limitations of UniGraph2,
we consider the general architecture of MGFMs, which typically consists of four core components: â¶ Modality-specific
encoding, â· Modality Interaction (including Graph Neural Network (GNN) processing), â¸ Modality Alignment,
and â¹ Modality Fusion. As detailed in our ablation studies
(See Fig. 1(c)&(d)), we involve a stepwise enhancement
of the UniGraph2 architecture, progressively endowing it
with capabilities for Modality Interaction (MI) and Modality
Alignment (MA). The results demonstrate that each module provides a positive contribution to the modelâ€™s overall
performance. This analysis forms the primary motivation
for our work: to develop dedicated mechanisms for these
two critical modules, tailored to address their challenges
at distinct granularities. Please refer to Appendix B.2 for
detailed implementation of the empirical study.
Motivated by the limitations mentioned above, in this work,
we propose PLANET, a novel framework that utilizes a
Divide-and-Conquer strategy to decouple the complexity
inherent in MGFM design across two distinct granularities. At the embedding granularity, we introduce the â¶
EDG for Modality Interaction. We formulate modality
interaction as a process of local semantic enrichment. This
module employs a domain-specialized gating mechanism to
adaptively extract and infuse topology-aware cross-modal
context, enhancing the modelâ€™s comprehension of intricate
inter-modality relationships at the fine-grained embedding
level. At the node granularity, we propose the â· NDR
for Modality Alignment. We view modality alignment as
a global semantic consensus problem. By constructing a
Discretized Semantic Representation Space (DSRS), this
module retrieves and anchors heterogeneous signals into a
unified space, explicitly enforcing robust alignment at the
coarse-grained node level.
In summary, our key contributions in PLANET are: (1)
New Perspective. To the best of our knowledge, we are
the first to systematically identify and address the critical
shortcomings of existing MGFMs with respect to lack of
modality interaction and alignment. (2) Novel Approach.
We propose PLANET, a novel framework that introduces a
paradigm of topology-aware modality interaction and modality alignment, offering a valuable reference for future designs in the MGFM domain. (3) SOTA Performance. Extensive experiments demonstrate the superiority of PLANET
across various graph-centric tasks and multimodal generative tasks.
2. Preliminaries
2.1. Notations and Problem Formulation
Consider a MAG denoted as G = (V, E, R), with |V| = N
nodes and |E| = M edges. R denotes the collection of raw
multimodal data associated with the nodes (e.g., raw text descriptions, images). Let M = {m1, m2, . . . , m|â„¦|} be the
set of available modalities, where |â„¦| denotes the number of
modalities. In this work, we operate under the assumption
that each node v âˆˆ V possesses complete features across
all modalities in M, ensuring a comprehensive multimodal
context for every entity. The raw data is transformed into feature X(m) âˆˆ R
NÃ—dm via Ï•m, where Ï•m denotes the frozen
modality-specific encoders for modality m (e.g., ViT (Dosovitskiy, 2020) for images, BERT (Devlin et al., 2019) for
texts). Consequently, we obtain a feature-transformed graph
G
â€² = (V, E, X ), where X = {X(m)}mâˆˆM serves as the
input node features for the subsequent learning process.
Based on G
â€²
, we aim to learn a unified embedding function
fÎ¸ following the standard paradigm: Pre-training stage.
We optimize fÎ¸ using self-supervised objectives (e.g., reconstruction tasks and contrastive tasks). Fine-tuning stage.
The pre-trained fÎ¸ is adapted with task-specific heads to
support diverse downstream tasks.
2.2. Multimodal Graph Learning
Multimodal Graph Learning (MGL) is the machine learning
technique on multimodal graphs (Peng et al., 2024). MGL
are widely employed across various domains, including biology (Gainza et al., 2020), chemistry (Guan et al., 2021),
knowledge graph (Chen et al., 2022; Zeng et al., 2023),
healthcare applications (Zheng et al., 2022) and recommendation systems (Wei et al., 2019; Tao et al., 2020; Yi et al.,
2022; Zhang et al., 2021). However, these existing models
2
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Multimodal
Attributed
Graph
Modality
Encoding
Modality
Interaction
Modality
Alignment
Modality
Fusion
x L layers
Pre-Training:
Self-supervised tasks
Finetuning:
Node
embedding
Modality
embedding
NC
LP
...
G2Text
G2Image
...
SMR
SR
CMR
...
MoE balance
loss
EDG for Modality Interaction
Expert 1
Expert 2
Expert n ...
Gate
Components
Attention
Mechanism
text
Attention
Mechanism
image
Query.
Query.
Key.
Value.
Key.
Value.
Text
emb
Image
emb
Data preprocessing + Modality-specific encoding
Original
Multimodal
Graph
Text Graph
Image Graph
Modalityspecific
Encoder
Quantized
Text emb
Quantized
Image emb
General Knowledge loss
DSRS: Discretized Semantic
Representation Space
Anchor
Discretization Retrieval
Step 1.1 Find Nearest Vector in DSRS
Step 1.2 Assign Vector
NDR for Modality Alignment
Dec Dec
SMR Loss: CMR Loss: SR Loss: Text modality
Image modality
Node
Center Node
(in k-hop neighbor
subgraphs)
Training
Assign vectors Frozen
... Node Embeddings
(After Modality Fusion)
Token embedding
Quantized embedding
Distance between
vectors
Overall Pipeline
Step 1. The MoE component selectively extracts relevant cross-modal semantics
from the 1-hop neighborhood. Step 2. Topology-aware Attention integrates these signals, using the target modality as Query and MoE outputs as Key/Values.
Step1. All node embeddings are mapped into DSRS. Step2. Computing General Knowledge loss for all assigned token embeddings. These
embeddings are used in Self-supervised tasks and downstream tasks.
Self-Supervised Tasks Legend In Framework
Topology-aware Modality
Interaction
Discretized Modality
Alignment
ğ’Ÿğ’Ÿğ“‚ğ“‚ğ“‚ğ“‚â€² ğ’ğ’ğ’ğ’
Dec
ğ’Ÿğ’Ÿ Decğ“‚ğ“‚
ğ’®ğ’®
Dec
ğ’Ÿğ’Ÿ Decğ“‚ğ“‚
ğ’®ğ’®
â„’s â„’ğ’¸ğ’¸ â„’ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡
Figure 2. Overall architecture of the proposed method: PLANET.
are inherently designed for specific tasks within specific
domains (e.g., user-item link predictions), lacking the generalization capability to transfer effectively to other domains
or downstream tasks.
2.3. Graph Foundation Models
Recently, Graph Foundation Models (GFMs) have gained
significant attention for their robust generalization capabilities (Liu et al., 2025b). Some approaches focused on
capturing universal structural patterns (Chen et al., 2025;
Yu et al., 2025; Sun et al., 2025). The majority of existing
GFMs concentrate on TAGs, which focus on aligning graph
structural representations with the semantic space (Tang
et al., 2024; Chen et al., 2024; Kong et al., 2025; Zhu et al.,
2025b; Li et al., 2024) and unifying diverse downstream
tasks across domains to achieve transferability (Liu et al.,
2024; Huang et al., 2023). However, these GFMs are not
primarily designed for MAGs.
Unlike previous works, UniGraph2 (He et al., 2025b) establishes a MGFM, but fails to effectively address the critical
issues of modality interaction and alignment (see Sec. 1).
Although a new study GraphGPT-O (Fang et al., 2025) attempts to incorporate cross-modal interaction, it employs a
non-topology-aware modality interaction mechanism within
an LLM-based framework restricted to generative tasks,
thereby suffering from prohibitive computational overhead
and limited versatility.
3. Methodology
3.1. Overview
We propose PLANET, a MGFM that uses a Divide-andConquer strategy to decouple modality interaction and
modality alignment (Fig. 2).
Modality Encoding. We use modality-specific encoders
to transform raw multimodal data into feature vectors. Following Hou et al. (2022), we employ a modality masking
strategy to these feature vectors (details in Appendix C.1),
yielding the masked feature vectors XËœ (m)
.
For feature dimension alignment across heterogeneous
modalities, we apply a set of modality-specific MLPs on
XËœ (m)
: H(0,m) = MLPm(XËœ (m)
). Subsequently, to preserve the inherent distribution of each modality, we process
H(0,m)
through independent, modality-specific GNNs (e.g.,
3
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
A plush
teddy bear
toy wearing
red ...
A small
plastic frog,
Children's
puzzle toy ...
A cute and soft
pink plush
rabbit pillow,
suitable for
girls to ...
plush
pillow
Expert 1: focus on category
Expert 2: focus on texture
influence text modality
1 2
3
Multimodal Attributed
Graph
plastic
toy
plush
toy
Node 1
Node 3
Node 2
Image
Image Image
MoE
Message passing
path
Node 1 Node 2
Node 3
Example of MoE-Based Cross-Modal Feature Extraction.
Figure 3. Illustration of expert-driven semantic extraction within
the EDG module. Taking the text modality as the target example,
we observe that the target text modality (in Node 3) correlates with
neighboring images (in Node 1,2) via diverse attributes. Our MoE
module is designed to capture these distinct semantic patterns
through specialized experts, enabling the precise extraction of
effective cross-modal mutual information.
Graph Transformer (Dwivedi & Bresson, 2020)) to obtain
the modality-specific embeddings H(spe,m)
.
Modality Interaction & Modality Alignment. We first
employ the EDG at the embedding granularity to infuse
topology-aware cross-modal context (Sec. 3.2). Subsequently, the NDR operates at the node granularity, anchoring
features into a DSRS for modality alignment. This process yields the aligned modality-level node representations
H(cross,m)
(Sec. 3.3).
Modality Fusion. To integrate modality specificity with
interactive semantics, we first fuse modality embeddings:
H(all,m) = H(spe,m)
|| H(cross,m)
. Specifically, for graphcentric tasks (e.g., node classification), we further concatenate multimodal representations to obtain the final node
embeddings: hi = âˆ¥mâˆˆMh
(all,m)
i
. The entire model is
optimized via a joint self-supervised objective (Sec. 3.4).
3.2. EDG for Modality Interaction
Following the Divide-and-Conquer strategy, the first challenge is to model modality interaction. We formulate this
as a process of local semantic enrichment at the embedding
granularity. Unlike previous works that aggregate modalities globally (He et al., 2025b), EDG operates within the
layer-wise propagation, allowing each nodeâ€™s modality embedding to dynamically absorb complementary semantics
from its neighbors before any global alignment is enforced.
MoE-Based Cross-Modal Feature Extraction. Intuitively,
in a MAG, the semantic representation of a node in one
modality (e.g., text) is often correlated with the complementary modalities (e.g., image) of its neighbors and itself. For
example, a textual description may correlate with the texture
and category of objects in images (See Fig. 3). Guided by
this observation, we employ a Mixture-of-Experts (MoE)
module to capture these diverse semantic patterns. Formally,
taking H(0,m)
as input, let h
(â„“,m)
i
denote the output embedding of node i for modality m at the l-th layer of the EDG
module.
For a neighbor node j and modality m. The effective crossmodal signal e
(â„“,m)
j
is computed as:
e
(â„“,m)
j =
X
K
k=1
G

n
(â„“,m)
j

k
Â· Ek

n
(â„“,m)
j

,
where n
(â„“,m)
j =



mâ€²Ì¸=m
h
(â„“âˆ’1,mâ€²
)
j
,
(1)
and Ek(Â·) represents the k-th expert network (implemented
as an MLP). Each expert specializes in discerning and extracting specific patterns of effective mutual information
from the neighbor node set. K denotes the total number of
expert. G(Â·)k is the gating score indicating the relevance of
the k-th expert, computed via a softmax gating network:
G

n
(â„“,m)
j

k
=
exp 
MLPg(n
(â„“,m)
j
)k

PK
kâ€²=1 exp 
MLPg(n
(â„“,m)
j
)kâ€²
, (2)
This ensures that the model dynamically selects experts to
capture the diverse semantic combinations of neighboring
modalities.
Topology-Aware Attention Mechanism. After mutual semantic information extraction, a Graph Transformer (Dwivedi & Bresson, 2020) layer is used to perform
topology-aware attention mechanism:
h
(â„“,m)
i = GTâ„“

q = h
(â„“âˆ’1,m)
i
; K, V = {e
(â„“,m)
j
}jâˆˆNi

, (3)
where Ni denotes the set of incoming neighbors for node i
(including node i itself), GTl(Â·) represents the l-th layer of
standard Graph Transformer. Crucially, distinct from standard cross-modal attention which typically operates within
isolated instances (Radford et al., 2021), our mechanism
explicitly integrates graph structural information into the
cross-modal interaction process by querying the complementary modalities of neighboring nodes. See Appendix C.2 for
the detailed formulations of our attention mechanism.
3.3. NDR for Modality Alignment
While the EDG module significantly enriches semantic representation by extracting and infusing cross-modal context,
we argue that a robust MGFM requires a more explicit constraint to bridge the inherent semantic gap between modalities. We formulate modality alignment here as a global
semantic consensus problem, to address this challenge, we
introduce the NDR module at the node granularity.
4
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Discretization Retrieval. We define Discretized Semantic
Representation Space (DSRS) as S = {s1, . . . , sC } containing C learnable latent vectors, referred to as tokens. Let
h
(L,m)
i
denote the final output representation of node i for
modality m from the EDG module. Each node embedding
is mapped to its nearest DSRS token:
h
(cross,m)
i = sc, where c= argmin
j
âˆ¥sj âˆ’ h
(L,m)
i
âˆ¥2, (4)
where âˆ¥ Â· âˆ¥2 denotes the Euclidean distance. This retrieval
step forces features from different modalities to cluster
around shared semantic space.
Text-Anchored General Knowledge Alignment. Inspired
by Liu et al. (2025a), language offers the most dense semantic information, we propose a General Knowledge Loss that
uses the Text modality (t) as an anchor. We align every other
modality m to the text modality in the DSRS:
Lgen=âˆ’
1
N(|â„¦|âˆ’1)
X
mÌ¸=t
XN
i=1
log
z
(t,m)
i,i
PN
j=1z
(t,m)
i,j
+log
z
(t,m)
i,i
PN
j=1z
(t,m)
j,i !
,
where z
(t,m)
i,j = exp 
sim(h
(cross,t)
i
, h
(cross,m)
j
)/Ï„
,
(5)
and Ï„ is a temperature parameter. This efficiently pulls the
quantized representations of the same node across modalities together while pushing distinct nodes apart.
VQ Objective. To align the latent distribution of modality
features with the quantized semantic space of DSRS, we
utilize the loss function (Van Den Oord et al., 2017):
LV Q =
1
N
X
N
i=1
X
mâˆˆM

âˆ¥sg[h
(cross,m)
i
] âˆ’ h
(L,m)
i
âˆ¥
2
2
+Î³âˆ¥h
(cross,m)
i âˆ’ sg[h
(L,m)
i
]âˆ¥
2
2

,
(6)
where sg[Â·] denotes the stop-gradient operator to handle the
non-differentiable quantization, and Î³ controls the commitment cost.
3.4. Self-Supervised Training
Feature Reconstruction. To ensure robust semantic understanding, we propose a dual reconstruction mechanism.
We minimize the error for recovering the masked input itself (Lself ) to capture modality-specific semantics, while
simultaneously enforcing cross-modal correlations by reconstructing features of complementary modalities:
Ls =
1
|â„¦|N
XN
i=1
X
mâˆˆM
âˆ¥DSMR
m (h
(all,m)
i
) âˆ’ x
(m)
i
âˆ¥
2
2, (7)
Lc =
PN
i=1
P
mÌ¸=mâ€² âˆ¥DCMR
mmâ€² (h
(all,m)
i
)âˆ’x
(mâ€²
)
i
âˆ¥
2
2
(|â„¦|
2 âˆ’ |â„¦|)N
,
(8)
where x
(m)
i
is the original unmasked feature vector output by the modality encoder, DSMR
m is the self modality
reconstruction decoder (MLP), and DCMR
mmâ€² is the cross
modality reconstruction decoder, both of which are implemented as MLPs. The total feature reconstruction loss is:
Lf eat = Ls + Î²interLc.
Structural Reconstruction. To preserve topological information, we employ a link reconstruction task. We use Ltopo
to denote the loss function, which enforces higher similarity
scores for connected edges compared to randomly sampled
negative pairs (details in Appendix C.3).
Total Objective. The final training objective combines all
losses:
L = Î²1Lfeat + Î²2Ltopo + Î²3Lgen + Î²4LV Q + Î²5Lload, (9)
where Î² terms are hyperparameters balancing the contribution of each component, Lload is load balancing loss
introduced in Appendix C.3.
3.5. Theoretical Analysis
3.5.1. WHY EDG CAPTURES SYNERGISTIC SEMANTICS?
Definition 3.1. Synergistic Features. Let G(A)
and G(B)
denote input graphs for modality A and B. It can be decomposed into independent modality-specific unique features
{UA, UB} and synergistic features {SA, SB}. {SA, SB}
satisfies the condition of zero mutual information under independent views, i.e., I(Y ; SA) â‰ˆ 0 and I(Y ; SB) â‰ˆ 0,
but positive interaction information under a joint view, i.e.,
I(Y ; SA, SB) > 0, where Y represents the latent semantic
information related to the data.
Theorem 3.2. Synergy Preservation via EDG. Let
Z
âˆ—
V anilla and Z
âˆ—
EDG denote the optimal representations
learned by a vanilla Multimodal Graph Encoder (e.g.,
MMGCN) and PLANET with the EDG module, respectively.
Under the compression constraint of the Information Bottleneck (Federici et al., 2020; Wu et al., 2020), provided that
the trade-off parameter Î² is sufficiently small, the information gap between the two representations satisfies:
I(Y ;Z
âˆ—
EDG) âˆ’ I(Y ;Z
âˆ—
V anilla) â‰¥ I(Y ; SA, SB | UA, UB) > 0.
(10)
Such a theorem demonstrates that vanilla encoders inevitably discard synergistic features {SA, SB} as noise. In
contrast, EDG explicitly captures them, yielding a strictly
larger information gain (proofs are shown in Appendix D).
3.5.2. HOW NDR ENHANCES ALIGNMENT EFFICIENCY?
Definition 3.3. Push-forward Measure. Let ÂµË†m =
1
N
PN
i=1 Î´
x
(m)
i
be the empirical measure of modality m,
where Î´ is the Dirac measure, N is the number of samples. Given a DSRS S and a quantization function Q(x) =
5
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 1. Main results on node classification tasks. We report Accuracy and F1-Macro for each dataset. Best results are highlighted in
bold , the second-best are marked with underline . All baselines are first pre-trained and then fine-tuned.
Method RedditS Movies Grocery Toys Ele-fashion Goodreads-NC
Acc F1-Macro Acc F1-Macro Acc F1-Macro Acc F1-Macro Acc F1-Macro Acc F1-Macro
GCN 92.44Â±0.62 87.34Â±1.22 52.26Â±0.74 42.35Â±1.23 78.60Â±0.41 64.90Â±1.09 77.63Â±0.72 75.54Â±0.97 85.33Â±0.06 68.01Â±0.23 78.44Â±0.06 68.12Â±0.16
MMGCN 90.27Â±0.34 84.22Â±0.73 53.41Â±1.03 41.66Â±2.03 82.56Â±0.49 73.83Â±0.93 80.02Â±0.64 76.36Â±1.23 86.59Â±0.08 68.85Â±0.35 83.22Â±0.10 71.28Â±0.22
MGAT 92.78Â±0.50 87.27Â±0.53 53.87Â±0.50 44.09Â±1.60 83.74Â±0.62 74.77Â±1.11 79.61Â±0.74 77.09Â±0.87 84.84Â±0.08 69.62Â±0.21 82.91Â±0.04 71.45Â±0.11
GRACE 93.01Â±0.53 88.39Â±1.12 48.09Â±0.97 37.18Â±1.33 70.83Â±0.81 60.69Â±1.05 72.82Â±0.66 69.09Â±0.63 83.58Â±0.11 70.09Â±0.47 74.96Â±0.06 70.09Â±0.11
GraphMAE2 92.81Â±0.44 87.93Â±0.37 50.08Â±0.77 38.68Â±1.63 76.24Â±0.60 66.74Â±1.33 75.11Â±0.52 71.80Â±0.50 83.32Â±0.31 65.92Â±0.59 74.15Â±0.22 69.20Â±0.28
RiemannGFM 91.63Â±0.45 85.20Â±1.13 52.80Â±0.43 40.74Â±1.25 82.62Â±0.46 74.90Â±1.55 77.85Â±0.45 74.84Â±0.60 87.07Â±0.20 70.45Â±1.32 78.13Â±0.17 70.73Â±0.24
GFT 93.02Â±0.37 87.00Â±2.03 51.33Â±0.67 28.14Â±1.82 76.80Â±2.22 59.11Â±3.02 79.52Â±0.58 76.00Â±0.92 87.14Â±0.22 70.33Â±1.24 75.93Â±0.41 66.18Â±0.30
SAMGPT 93.11Â±0.19 87.12Â±0.64 50.25Â±0.28 34.21Â±1.37 76.41Â±0.54 63.40Â±0.88 73.81Â±0.41 67.12Â±0.73 83.81Â±0.11 69.73Â±0.39 74.29Â±0.11 66.57Â±0.29
UniGraph2 93.65Â±0.17 87.91Â±0.68 53.02Â±0.53 43.43Â±1.86 82.10Â±0.37 73.93Â±1.37 79.00Â±0.59 76.02Â±0.78 87.06Â±0.18 69.80Â±0.82 79.06Â±0.27 68.74Â±0.36
PLANET 96.62Â±0.22 92.44Â±0.43 57.06Â±0.61 47.49Â±1.23 85.16Â±0.88 77.23Â±0.86 81.22Â±0.50 77.55Â±0.75 87.37Â±0.12 70.74Â±0.78 84.16Â±0.07 74.43Â±0.18
Rel-Improv. â†‘ 3.17% â†‘ 4.58% â†‘ 5.92% â†‘ 7.71% â†‘ 1.70% â†‘ 3.11% â†‘ 1.50% â†‘ 0.60% â†‘ 0.26% â†‘ 0.41% â†‘ 1.13% â†‘ 4.17%
Table 2. Link prediction and few-shot link classification results. We report MRR for link prediction, accuracy for few-shot link
classification tasks.
Method
Link Prediction Few-shot Link Classification
Amazon
-Sports
Amazon
-Cloth
Goodreads
-LP
Amazon-Sports-2Way Amazon-Cloth-2Way
10-shot 5-shot 3-shot 10-shot 5-shot 3-shot
MMGCN 23.44Â±0.43 17.74Â±0.38 20.73Â±0.48 56.66Â±1.60 53.70Â±1.20 55.86Â±2.96 67.27Â±4.20 68.61Â±5.38 64.36Â±4.07
MGAT 21.74Â±0.96 15.47Â±0.32 21.82Â±0.53 57.92Â±1.80 56.55Â±2.69 54.92Â±2.40 68.66Â±2.94 70.34Â±3.36 67.05Â±1.45
GRACE 25.31Â±0.16 18.27Â±0.15 19.30Â±0.27 58.50Â±1.43 57.94Â±2.58 56.42Â±1.36 64.96Â±1.86 64.94Â±1.09 62.36Â±2.75
GraphMAE2 24.54Â±0.30 18.69Â±0.21 19.99Â±0.19 56.05Â±1.70 54.36Â±1.65 53.28Â±3.09 62.33Â±1.78 61.39Â±1.71 60.95Â±1.61
GFM
RiemannGFM 21.92Â±0.51 19.20Â±0.44 22.03Â±0.67 53.68Â±1.24 53.73Â±2.03 54.07Â±1.46 66.20Â±5.94 62.66Â±1.75 63.45Â±4.07
GFT 22.04Â±0.62 17.63Â±0.59 20.16Â±1.21 55.73Â±3.36 56.86Â±2.30 56.75Â±2.63 65.37Â±4.03 66.66Â±2.10 63.70Â±2.84
SAMGPT 24.09Â±0.22 16.41Â±0.20 24.91Â±0.38 60.48Â±4.25 59.00Â±3.56 59.41Â±3.62 74.25Â±1.75 74.20Â±1.60 72.61Â±1.90
MGFM
UniGraph2 27.09Â±0.13 19.31Â±0.35 19.44Â±0.19 65.08Â±3.17 64.27Â±2.74 60.83Â±3.90 73.77Â±1.91 71.28Â±4.20 73.44Â±1.57
PLANET 27.51Â±0.14 20.25Â±0.27 27.62Â±0.25 67.84Â±1.38 64.36Â±3.05 62.89Â±3.99 75.44Â±2.84 75.22Â±1.20 74.03Â±4.12
Rel-Improv. â†‘ 1.55% â†‘ 4.87% â†‘ 10.87% â†‘ 4.24% â†‘ 0.14% â†‘ 3.39% â†‘ 1.60% â†‘ 1.37% â†‘0.80%
arg mineâˆˆS âˆ¥x âˆ’ eâˆ¥2, we define the Push-forward Measure of modality m as Î½Ë†m = Q#ÂµË†m =
1
N
PN
i=1 Î´Q(x
(m)
i
)
.
Theorem 3.4. Efficient Alignment via NDR. Assume the
feature space is bounded. The alignment error between the
empirical distribution of modality m (ÂµË†m) and the anchor
text modality t (ÂµË†t) is bounded by:
W1(Ë†Âµm, ÂµË†t)â‰¤Exâˆ¼ÂµË†mâˆ¥x âˆ’ Q(x)âˆ¥2 + Ezâˆ¼ÂµË†t âˆ¥z âˆ’ Q(z)âˆ¥2+
W1(Î½
âˆ—
m, Î½
âˆ—
t ) + O

C
âˆš
N

,
(11)
where W1(Â·, Â·) denotes the 1-Wasserstein distance between
distributions, C represents the DSRS size, and W1(Î½
âˆ—
m, Î½âˆ—
t
)
represents the intrinsic bias between modalities.
This proves that projecting features into DSRS accelerates
the alignment convergence rate from O(N âˆ’1/d) in continuous spaces to O(N âˆ’1/2
). Additionally, minimizing LV Q
reduces the quantization error terms in Eq. (11), explicitly
tightening the upper bound to ensure robust alignment.
4. Experiments
To validate the superiority of PLANET, we raise several
questions: Q1: Does PLANET consistently outperform
SOTA baselines across standard graph-centric tasks (i.e.,
node classification and link prediction) under both supervised learning and few-shot learning scenarios? Q2: How do
the proposed EDG and NDR modules contribute to achieving topology-aware modality interaction and robust modality alignment, respectively? Q3: Can PLANET effectively
support downstream generative tasks, thereby demonstrating
the robust generalization capabilities and versatile applicability of a foundation model? Q4: How does PLANET
fare in terms of computational efficiency compared to existing GFMs? The implementation details are introduced in
Appendix E.
4.1. Graph-Centric Tasks
Experimental Settings. To answer Q1, we conduct extensive evaluations on node classification and link prediction
tasks under both supervised learning and few-shot learning scenarios. We compare PLANET with 9 strong baselines, which can be categorized into five distinct groups: (1)
Vanilla GNNs: GCN (Kipf & Welling, 2017). (2) Multimodal Graph Models: Including MMGCN (Wei et al.,
2019) and MGAT (Tao et al., 2020). (3) Self-supervised
Graph Learning Models: Comprising contrastive learning
method, GRACE (Zhu et al., 2020) and generative learning method, GraphMAE2 (Hou et al., 2023). (4) Graph
6
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 3. Few-shot node classification results. We report accuracy for node classification tasks.
Method Grocery-5way Ele-fashion-5way Goodreads-NC-5way
10-shot 5-shot 3-shot 10-shot 5-shot 3-shot 10-shot 5-shot 3-shot
MMGCN 53.73Â±3.51 50.30Â±3.13 48.13Â±2.65 60.87Â±3.08 57.05Â±2.42 54.37Â±2.99 56.42Â±2.84 54.10Â±2.90 52.93Â±2.94
MGAT 55.53Â±2.98 54.07Â±3.18 50.83Â±2.88 62.82Â±2.18 61.10Â±2.20 60.38Â±2.57 58.22Â±3.40 57.05Â±2.35 53.15Â±3.09
GRACE 62.22Â±2.89 59.22Â±3.50 57.28Â±4.91 64.72Â±3.04 59.90Â±2.73 55.49Â±4.10 59.20Â±2.00 61.53Â±3.14 56.84Â±4.65
GraphMAE2 58.00Â±4.26 54.60Â±3.83 51.90Â±5.76 63.65Â±3.48 60.00Â±3.53 57.35Â±3.05 49.83Â±2.60 46.30Â±2.95 43.20Â±2.70
GFM
RiemannGFM 67.14Â±2.02 66.23Â±2.73 63.63Â±2.48 61.73Â±2.90 60.24Â±3.55 59.13Â±3.71 60.54Â±3.92 57.17Â±3.83 54.29Â±4.10
GFT 63.12Â±4.07 64.60Â±3.54 61.45Â±2.22 62.28Â±2.62 61.38Â±3.41 59.08Â±3.95 51.62Â±4.73 50.95Â±4.35 48.70Â±4.96
SAMGPT 66.47Â±10.67 62.33Â±10.49 52.73Â±12.69 61.27Â±10.82 61.13Â±9.32 54.20Â±10.78 51.80Â±8.65 47.00Â±8.35 42.53Â±6.79
MGFM
UniGraph2 66.27Â±3.11 61.25Â±3.09 60.05Â±4.13 60.38Â±2.08 58.73Â±2.95 53.98Â±4.07 63.80Â±4.02 61.55Â±3.97 59.67Â±4.23
PLANET 81.93Â±3.48 79.88Â±3.27 77.85Â±4.06 74.85Â±3.73 72.97Â±3.55 70.50Â±4.37 69.18Â±3.90 67.59Â±4.68 63.62Â±4.11
Rel-Improv. â†‘ 22.03% â†‘ 20.61% â†‘ 22.35% â†‘ 15.65% â†‘ 18.88% â†‘ 16.76% â†‘ 8.43% â†‘ 9.81% â†‘ 6.62%
Foundation Models: Including SAMGPT (Yu et al., 2025)
and RiemannGFM (Sun et al., 2025), which focus on learning unified graph structures, and GFT (Wang et al., 2024b),
which targets TAGs. (5) Multimodal Graph Foundation
Models: Specifically UniGraph2 (He et al., 2025b), a novel
baseline explicitly designed for processing MAGs. Please
refer to Appendix E for detailed implementation of baselines.
Supervised Learning. Table 1 and 2 show the results of
node classification and link prediction in supervised learning. The results indicate that PLANET consistently outperforms baselines across all datasets. We attribute this superior
performance to the synergistic collaboration between the
EDG and NDR modules operating at distinct granularities.
By jointly facilitating efficient topology-aware modality interaction and alignment, PLANET ensures the generation
of high-quality node embeddings.
Few-Shot Learning. Tables 2 and 3 present the results for
few-shot node classification and link prediction, respectively.
In these challenging low-resource scenarios, PLANET continues to outperform baselines across all datasets. Most
notably, in few-shot node classification tasks, PLANET
achieves remarkable performance gains (15.68%). These
results underscore the modelâ€™s robust learning and generalization capabilities under conditions of severe data scarcity,
providing evidence that our pre-training paradigm effectively enables the model to acquire comprehensive prior
knowledge, which can be efficiently transferred to downstream tasks with minimal supervision.
4.2. Ablation Study
To address Q2, we conduct rigorous ablation studies removing core components of PLANET, as shown in Table 4.
â¶ w/o. MoE Component. We remove the MoE mechanism.
Here, neighboring multimodal features are fed directly into
the attention mechanism. The result confirms that the MoE
is essential for extracting effective cross-modal signals before modality interaction. â· w/o. MoE + Attention Mechanism. We investigate the impact of topology-aware modality
Table 4. Ablation studies on PLANET key components.
Toys RedditS Ele-fashion Amazon-Sports
EDG for Modality Interaction
w/o. MoE 77.77Â±0.64 95.19Â±0.30 85.33Â±0.27 25.92Â±0.16
w/o. MoE+AM 76.33Â±0.54 93.01Â±0.16 84.52Â±0.14 24.76Â±0.13
NDR for Modality Alignment
w/o. DSRS 78.53Â±0.80 95.12Â±0.33 85.93Â±0.18 26.19Â±0.22
w/o. Lgen 77.85Â±0.73 94.97Â±0.31 84.71Â±0.22 25.38Â±0.19
PLANET 81.22Â±0.50 96.62Â±0.22 87.37Â±0.12 27.51Â±0.14
interaction by replacing the entire EDG module with standard, independent Graph Transformers for each modality.
The significant performance degradation validates the effectiveness of explicitly modeling the topology-aware interplay
between modalities. â¸ w/o. DSRS. We remove the DSRS
and the associated Discretization Retrieval, directly applying the General Knowledge Loss (Lgen) to the continuous
embeddings h
(L,m)
i
. The resulting performance drop confirms its critical role in modality alignment by projecting
distinct modalities into a unified semantic space. â¹ w/o.
General Knowledge Loss. We retain the DSRS structure but
disable the text-anchored General Knowledge Loss (Lgen).
Results show that without this signal, the model fails to
effectively bridge the semantic gap between the quantized
tokens of different modalities, leading to suboptimal alignment.
4.3. Multimodal Generative Tasks
Experimental Settings. Leveraging the rich cross-domain
semantic information and universal topological patterns
learned during pre-training, PLANET exhibits strong potential for generative applications. To address Q3, we
conduct evaluations on two distinct tasks: â¶ Graph-toText (G2Text) Generation. This task aims to generate a
comprehensive textual description for a target node, conditioned on the graph structure and multimodal context.
We strictly follow the evaluation settings of MMGL (Yoon
et al., 2023). Specifically, MMGL relies on frozen CLIP
encoders, and we replace these embeddings with the text
7
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
MMGL MMGL-UniGraph2 MMGL-PLANET
(a) BLEU-3 (b) BLEU-4 (c) ROUGE-L (d) CIDEr
Flickr30k Grocery Flickr30k Grocery Flickr30k Grocery Flickr30k Grocery
Figure 4. G2Text generation results. We report BLEU-3, BLEU-4, ROUGE-L, and CIDEr on the Flickr30k and Grocery datasets.
InstructG2I InstructG2I-UniGraph2 InstructG2I-PLANET
CLIP score
DINOv2 score
(a) Goodreaders-NC: History (b) Goodreaders-NC: Children (c) Ele-fashion: Jewelry (d) Ele-fashion: Shoes
Figure 5. G2Image generation results. We report CLIP scores in red and DINOv2 scores in blue across four categories selected from the
Goodreads-NC and Ele-fashion datasets.
and image embeddings generated by PLANET. â· Graphto-Image (G2Image) Generation. This task focuses on
synthesizing an image for a target node based on a text
prompt and neighbor image context. Adopting the framework of InstructG2I (Jin et al., 2024), we replace the image
features extracted by the pre-trained CLIP image encoder
with the topology-enriched image embeddings produced
by PLANET. Implementation details are provided in Appendix E.3.
Results. Fig. 4 and 5 shows the results of G2Text and
G2Image tasks, respectively. PLANET consistently outperforms baselines across all metrics. We attribute this to
our Divide-and-Conquer strategy, which produces superior
embeddings through modality interaction and alignment.
4.4. Computational Efficiency Analysis
To answer Q4, we conducted an efficiency analysis on the
large-scale Goodreads-NC dataset. We evaluated the total pre-training and fine-tuning time of various baselines.
For task-specific models (i.e., GAT, MMGCN, MGAT), we
record their End-to-End (E2E) training time. Additionally, we monitor the GPU memory usage across different
stages. Results (Fig. 6) demonstrate that PLANET achieves
a superior efficiency-performance trade-off, validating the
scalability of our Divide-and-Conquer design. PLANET
utilizes sufficient memory for pre-training, its fine-tuning
memory usage is minimal since only the linear classifier
is trained. This indicates that our model enables resourceefficient adaptation for downstream tasks.
Pre-training
Fine-tuning / E2E
Memory Usage (MB)
(a) Total Time Consumption (b) Memory Usage
Acc (%)
Time (h)
MMGCN
 0.5h MGAT
0.6h
PLANET
4.5h
UniGraph2
1.7h
RiemannGFM
29.9h
GFT
SAMGPT 9.0h
4.0h
GAT
0.3h
Figure 6. Efficiency comparison between models on GoodreadsNC. (a) Total time consumption. (b) GPU memory usage during
different stages.
5. Conclusion
In this work, we propose PLANET to resolve the critical limitations of existing MGFMs in modality interaction
and alignment. Leveraging a Divide-and-Conquer strategy, the EDG module couples expert-driven extraction with
topology-aware attention to facilitate modality interaction at
embedding level, and the NDR module bridges the semantic
gap by mapping multimodal representations into a unified
Discretized Semantic Representation Space at node level.
Theoretical analysis and extensive experiments confirm the
superiority of PLANET, which achieves state-of-the-art performance across diverse graph-centric and multimodal generative tasks, establishing a robust and scalable framework
for learning on MAGs. In future work, we aim to extend
PLANET to incorporate richer modalities like audio and
video, broadening the applicability of MGFMs in web-scale
scenarios.
8
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none of which we feel must be
specifically highlighted here.
References
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450, 2016.
Chen, J., Zuo, H., Wang, H. P., Miao, S., Li, P., and Ying,
R. Towards a universal graph structural encoder. arXiv
preprint arXiv:2504.10917, 2025.
Chen, R., Zhao, T., Jaiswal, A., Shah, N., and Wang, Z.
Llaga: Large language and graph assistant. arXiv preprint
arXiv:2402.08170, 2024.
Chen, X., Zhang, N., Li, L., Deng, S., Tan, C., Xu, C.,
Huang, F., Si, L., and Chen, H. Hybrid transformer
with multi-level fusion for multimodal knowledge graph
completion. In Proceedings of the 45th international
ACM SIGIR conference on research and development
in information retrieval, pp. 904â€“915, 2022.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
Bert: Pre-training of deep bidirectional transformers
for language understanding. In Proceedings of the
2019 conference of the North American chapter of
the association for computational linguistics: human
language technologies, volume 1 (long and short papers),
pp. 4171â€“4186, 2019.
Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
Dwivedi, V. P. and Bresson, X. A generalization
of transformer networks to graphs. arXiv preprint
arXiv:2012.09699, 2020.
Fang, Y., Jin, B., Shen, J., Ding, S., Tan, Q., and Han, J.
Graphgpt-o: Synergistic multimodal comprehension and
generation on graphs. In Proceedings of the Computer
Vision and Pattern Recognition Conference, pp. 19467â€“
19476, 2025.
Federici, M., Dutta, A., Forre, P., Kushman, N., and Akata, Â´
Z. Learning robust representations via multi-view information bottleneck. arXiv preprint arXiv:2002.07017,
2020.
Fedus, W., Zoph, B., and Shazeer, N. Switch transformers:
Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research,
23(120):1â€“39, 2022.
Gainza, P., Sverrisson, F., Monti, F., Rodola, E., Boscaini,
D., Bronstein, M. M., and Correia, B. E. Deciphering
interaction fingerprints from protein molecular surfaces
using geometric deep learning. Nature methods, 17(2):
184â€“192, 2020.
Guan, Y., Coley, C. W., Wu, H., Ranasinghe, D., Heid, E.,
Struble, T. J., Pattanaik, L., Green, W. H., and Jensen,
K. F. Regio-selectivity prediction with a machine-learned
reaction representation and on-the-fly quantum mechanical descriptors. Chemical science, 12(6):2198â€“2208,
2021.
He, Y., Sui, Y., He, X., and Hooi, B. Unigraph: Learning a unified cross-domain foundation model for textattributed graphs. Proceedings of the ACM SIGKDD
Conference on Knowledge Discovery and Data Mining,
KDD, 2025a.
He, Y., Sui, Y., He, X., Liu, Y., Sun, Y., and Hooi, B. Unigraph2: Learning a unified embedding space to bind
multimodal graphs. In Proceedings of the ACM on Web
Conference, WWW, 2025b.
Hou, Z., Liu, X., Cen, Y., Dong, Y., Yang, H., Wang, C.,
and Tang, J. Graphmae: Self-supervised masked graph
autoencoders. In Proceedings of the 28th ACM SIGKDD
conference on knowledge discovery and data mining, pp.
594â€“604, 2022.
Hou, Z., He, Y., Cen, Y., Liu, X., Dong, Y., Kharlamov, E.,
and Tang, J. Graphmae2: A decoding-enhanced masked
self-supervised graph learner. In Proceedings of the ACM
web conference 2023, pp. 737â€“746, 2023.
Huang, Q., Ren, H., Chen, P., Krzmanc, G., Zeng, D., Liang, Ë‡
P. S., and Leskovec, J. Prodigy: Enabling in-context
learning over graphs. Advances in Neural Information
Processing Systems, 36:16302â€“16317, 2023.
Jin, B., Pang, Z., Guo, B., Wang, Y.-X., You, J., and
Han, J. Instructg2i: Synthesizing images from multimodal attributed graphs. Advances in Neural Information
Processing Systems, 37:117614â€“117635, 2024.
Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. In International
Conference on Learning Representations, ICLR, 2017.
Kong, L., Feng, J., Liu, H., Huang, C., Huang, J., Chen, Y.,
and Zhang, M. Gofa: A generative one-for-all model for
joint graph language modeling. International Conference
on Learning Representations, ICLR, 2025.
Li, Y., Wang, P., Li, Z., Yu, J. X., and Li, J. Zerog: Investigating cross-dataset zero-shot transferability in graphs. In
Proceedings of the 30th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, pp. 1725â€“1735,
2024.
9
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Liu, F., Li, Z., Yin, Q., Huang, J., Luo, J., Thakur, A., Branson, K., Schwab, P., Yin, B., Wu, X., et al. A multimodal
multidomain multilingual medical foundation model for
zero shot clinical diagnosis. npj Digital Medicine, 8(1):
86, 2025a.
Liu, H., Feng, J., Kong, L., Liang, N., Tao, D., Chen, Y., and
Zhang, M. One for all: Towards training one graph model
for all classification tasks. International Conference on
Learning Representations, ICLR, 2024.
Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai,
T., Fang, Y., Sun, L., Yu, P. S., et al. Graph foundation models: Concepts, opportunities and challenges.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2025b.
Peng, C., He, J., and Xia, F. Learning on multimodal graphs:
A survey. arXiv preprint arXiv:2402.05322, 2024.
Qu, L., Liu, M., Wu, J., Gao, Z., and Nie, L. Dynamic modality interaction modeling for image-text retrieval. In Proceedings of the 44th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pp. 1104â€“1113, 2021.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J., et al. Learning transferable visual models from natural language supervision. In International conference on
machine learning, pp. 8748â€“8763. PmLR, 2021.
Sun, L., Huang, Z., Zhou, S., Wan, Q., Peng, H., and Yu, P.
Riemanngfm: Learning a graph foundation model from
riemannian geometry. In Proceedings of the ACM on
Web Conference, WWW, 2025.
Tang, J., Yang, Y., Wei, W., Shi, L., Su, L., Cheng, S.,
Yin, D., and Huang, C. Graphgpt: Graph instruction
tuning for large language models. In Proceedings of the
47th International ACM SIGIR Conference on Research
and Development in Information Retrieval, pp. 491â€“500,
2024.
Tao, Z., Wei, Y., Wang, X., He, X., Huang, X., and Chua,
T.-S. Mgat: Multimodal graph attention network for recommendation. Information Processing & Management,
57(5):102277, 2020.
Van Den Oord, A., Vinyals, O., et al. Neural discrete
representation learning. Advances in neural information
processing systems, NeurIPS, 30, 2017.
Villani, C. et al. Optimal transport: old and new, volume
338. Springer, 2008.
Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen,
K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing
vision-language modelâ€™s perception of the world at any
resolution. arXiv preprint arXiv:2409.12191, 2024a.
Wang, Z., Zhang, Z., Chawla, N., Zhang, C., and Ye, Y.
Gft: Graph foundation model with transferable tree vocabulary. Advances in Neural Information Processing
Systems, NeurIPS, 2024b.
Weed, J. and Bach, F. Sharp asymptotic and finite-sample
rates of convergence of empirical measures in wasserstein
distance. Bernoulli, 25(4A):2620â€“2648, 2019.
Wei, Y., Wang, X., Nie, L., He, X., Hong, R., and Chua,
T.-S. Mmgcn: Multi-modal graph convolution network
for personalized recommendation of micro-video. In
Proceedings of the 27th ACM international conference
on multimedia, pp. 1437â€“1445, 2019.
Wu, T., Ren, H., Li, P., and Leskovec, J. Graph information
bottleneck. Advances in Neural Information Processing
Systems, 33:20437â€“20448, 2020.
Xia, L. and Huang, C. Anygraph: Graph foundation model
in the wild. arXiv preprint arXiv:2408.10700, 2024.
Xia, L., Kao, B., and Huang, C. Opengraph: Towards open
graph foundation models. In Findings of the Association
for Computational Linguistics: EMNLP 2024, 2024.
Yan, H., Li, C., Yin, J., Yu, Z., Han, W., Li, M., Zeng,
Z., Sun, H., and Wang, S. When graph meets multimodal: benchmarking and meditating on multimodal
attributed graph learning. In Proceedings of the 31st
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining V. 2, pp. 5842â€“5853, 2025.
Yi, Z., Wang, X., Ounis, I., and Macdonald, C. Multimodal graph contrastive learning for micro-video recommendation. In Proceedings of the 45th international
ACM SIGIR conference on research and development in
information retrieval, pp. 1807â€“1811, 2022.
Yoon, M., Koh, J. Y., Hooi, B., and Salakhutdinov, R. Multimodal graph learning for generative tasks. arXiv preprint
arXiv:2310.07478, 2023.
Yu, X., Gong, Z., Zhou, C., Fang, Y., and Zhang, H. Samgpt:
Text-free graph foundation model for multi-domain pretraining and cross-domain adaptation. In Proceedings of
the ACM on Web Conference, WWW, 2025.
Zeng, Y., Jin, Q., Bao, T., and Li, W. Multi-modal
knowledge hypergraph for diverse image retrieval.
In Proceedings of the AAAI conference on artificial
intelligence, volume 37, pp. 3376â€“3383, 2023.
Zhang, J., Zhu, Y., Liu, Q., Wu, S., Wang, S., and Wang,
L. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM international
conference on multimedia, pp. 3872â€“3880, 2021.
10
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Zheng, S., Zhu, Z., Liu, Z., Guo, Z., Liu, Y., Yang, Y.,
and Zhao, Y. Multi-modal graph learning for disease
prediction. IEEE Transactions on Medical Imaging, 41
(9):2207â€“2216, 2022.
Zhu, J., Zhou, Y., Qian, S., He, Z., Zhao, T., Shah,
N., and Koutra, D. Mosaic of modalities: A comprehensive benchmark for multimodal graph learning.
In Proceedings of the Computer Vision and Pattern
Recognition Conference, pp. 14215â€“14224, 2025a.
Zhu, Y., Xu, Y., Yu, F., Liu, Q., Wu, S., and Wang, L. Deep
graph contrastive representation learning. arXiv preprint
arXiv:2006.04131, 2020.
Zhu, Y., Shi, H., Wang, X., Liu, Y., Wang, Y., Peng, B.,
Hong, C., and Tang, S. Graphclip: Enhancing transferability in graph foundation models for text-attributed
graphs. In Proceedings of the ACM on Web Conference,
WWW, 2025b.
11
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 5. Statistics of MAG datasets.
Domain Dataset Avg. #Nodes Avg. #Edges #Graphs Task #Classes #Pretrain Weights
E-commerce
Network
Movies 16,672 218,390 1 NC 20 5.0
Toys 20,695 126,886 1 NC 18 5.0
Grocery 17,074 171,340 1 NC 20 5.0
Amazon-Sports 50,250 356,202 1 LP â€“ 1.0
Amazon-Cloth 125,839 951,271 1 LP â€“ 1.0
Ele-fashion 97,766 199,602 1 NC 12 1.0
Social Network Reddit-S 15,894 566,160 1 NC 20 10.0
Flickr30k 31,783 181,551 1 â€“ â€“ 0.0
Knowledge Graph MM-CoDEx-s 1,383 15,884 1 KGC â€“ 20.0
MM-CoDEx-m 7,697 52,840 1 KGC â€“ 20.0
Books Network Goodreads-NC 685,294 7,235,048 1 NC 11 0.5
Goodreads-LP 636,502 3,437,017 1 LP â€“ 0.5
A. Datasets
Dataset. Detailed information of each dataset is presented in Table 5.
â€¢ MAGB Datasets. We select four representative graphs from the MAGB benchmark (Yan et al., 2025). Movies, Toys
and Grocery are e-commerce networks constructed from Amazon. Nodes represent products, and edges indicate
also-bought or also-viewed co-purchasing relationships. Each node is associated with a product description and a
product image. Reddit-S is a social network graph derived from the Reddit platform. Nodes represent user posts
containing both text and images. Edges connect posts commented on by the same user, reflecting shared user interests.
â€¢ MM-GRAPH Datasets. We incorporate seven datasets from the MM-GRAPH benchmark (Zhu et al., 2025a).
Amazon-Sports, Amazon-Cloth and Ele-fashion are e-commerce networks where nodes represent items associated
with product titles and images. Goodreads-NC and Goodreads-LP are books networks enriched with cover and
descriptions. Edges represent user co-interactions, linking books read or liked by the same users. MM-CoDEx-s and
MM-CoDEx-m are knowledge graphs where entities contain Wikipedia texts and images. Edges denote semantic
relations (e.g., born in) between entities.
â€¢ Flickr30k. Flickr30k is a social network. Notably, Flickr30k lacks an intrinsic graph structure. To adapt it for graph
learning, we construct a k-NN graph, where edges are established between nodes with high feature similarity to capture
latent semantic correlations. Nodes represent images associated with five descriptions.
Dataset Split. For the MAGB datasets and Flickr30k, each dataset is randomly partitioned into training sets (60%),
validation sets (20%) and testing sets (20%). For the MM-GRAPH datasets, we strictly adhere to the official data splits
provided in the original benchmark (Zhu et al., 2025a).
B. Detailed Implementation of Empirical Study
B.1. Detailed Implementation of Empirical Study 1
The first empirical study(Fig. 1(a)&(b)) is designed to validate the benefits of incorporating multimodal information and the
superiority of MGFMs approaches. we conducted a comparative study involving both traditional MGL models (MMGCN,
MGAT), Graph Foundation Models (RiemannGFM, SAMGPT) and Multimodal Graph Foundation Models (UniGraph2).
Input Modality Settings. We standardized the feature dimensions and encoding processes across all settings. Specifically,
both text data and image data are encoded through Qwen2-VL-7B-Instruct (Wang et al., 2024a). The output features are
in a fixed dimension of 3,584. For Text or Image settings, These settings represent single-modality scenarios. we adopted
a duplication strategy to ensure compatibility with model architectures which need dual modality inputs. For instance,
in the Text setting, we utilize the text embeddings as the primary input and duplicate them to serve as the input for the
12
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
second modality (i.e., replacing the image features with text features). For the Text+Image setting, we provide models with
comprehensive multimodal information (i.e., text embeddings and image embeddings).
Model Adaptation. For models inherently designed to handle MAGs, including MMGCN, MGAT, and UniGraph2, we
retain their original architectures without modification. For GFMs that are not naturally designed to process MAGs, including
RiemannGFM and SAMGPT, we concatenated the embeddings from the distinct modalities (e.g., for the Text+Image setting,
we concatenated the text and image embeddings) to form a unified node feature embedding.
Results Analysis. As shown in Fig. 1(a)&(b), the Text+Image setting consistently outperforms single-modality baselines,
corresponding to the conclusion: â¶ The use of multiple modalities consistently improve model performance. Comparing
UniGraph2 with RiemannGFM and SAMGPT, UniGraph2 significantly outperforms general GFMs on both dataset across
all settings, corresponding to the conclusion: â· MGFMs significantly outperform GFMs.
B.2. Detailed Implementation of Empirical Study 2
This empirical study (Fig. 1(c)&(d)) is designed to validate the limitations of recent MGFMs, exemplified by the state-ofthe-art UniGraph2.
Modality Interaction (MI). The original UniGraph2 employs an â€œearly fusionâ€ strategy, where embeddings from different
modalities are aggregated via weighted sum before entering graph encoder. To realize MI, we removed the early fusion
mechanism and introduced a naive layer-wise interaction strategy. Specifically, in each Graph Encoder layer , we allow
information to flow from one modality mâ€²
to another modality m via a linear projection:
h
(â„“,m) = GNNLayer(l)
m

h
(â„“âˆ’1,m)

+
Î±
|â„¦| âˆ’ 1
X
mâ€²Ì¸=m
W(l)
mâ€²m Â· GNNLayer(l)
mâ€²

h
(â„“âˆ’1,mâ€²
)

(12)
where W(l)
mâ€²m is a learnable projection matrix in layer l, capturing the relationship between modalities mâ€²
and m. Î± is a
hyperparameter.
MI+Modality Alignment (MA). Building upon the MI, we incorporated the Symmetric InfoNCE Loss (Radford et al.,
2021) to enforce explicit modality alignment. This contrastive objective pulls representations of the same node across
different modalities closer in the latent space.
C. Detailed Formulations
C.1. Modality Masking
Inspired by Hou et al. (2022), we employ a modality masking strategy. Formally, for each node vi âˆˆ VËœ and modality m âˆˆ â„¦,
we have:
xËœ
(m)
i = x
(m)
i âŠ™ b
(m)
i
, (13)
where b
(m)
i âˆˆ {0, 1}
dm is a binary mask vector sampled from a Bernoulli distribution, V âŠ‚ V Ëœ is a sampled subset of nodes,
and âŠ™ denotes the element-wise product. This process encourages the model to recover missing feature dimensions using
contextual information from the graph and other modalities.
C.2. Topology-Aware Attention Mechanism
In Sec. 3.2, we abstract the topology-aware attention mechanism as a generic operator GTl(Â·). Here, we provide its detailed
mathematical formulation. We strictly follow the implementation of Graph Transformer (Dwivedi & Bresson, 2020), but
change the query,key and value vectors. The output of the multi-head attention is computed as:
hË†
(â„“,m)
i = O(â„“,m)
H



k=1
ï£«
ï£­
X
jâˆˆNi
Î±
(â„“,m)
ij,k V
(â„“,m)
k
e
(â„“,m)
j
ï£¶
ï£¸ , (14)
where H represents the number of heads. âˆ¥ denotes the concatenation operation, Ni denotes the set of incoming neighbors
for node i, and O(â„“,m)
is the output projection matrix. For the k-th head, V
(â„“,m)
k maps the expert-distilled neighbor
signal e
(â„“,m)
j
to the value vector. The attention coefficient Î±
(â„“,m)
ij,k determines the importance of neighbor jâ€™s cross-modal
13
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
information to node i:
Î±
(â„“,m)
ij,k = softmaxj
ï£«
ï£¬ï£­

Q
(â„“,m)
k h
(â„“âˆ’1,m)
i
âŠ¤ 
K
(â„“,m)
k
e
(â„“,m)
j

âˆš
dk
ï£¶
ï£·ï£¸ , (15)
where dk is the dimension of each head. The projection matrices Q
(â„“,m)
k
, K
(â„“,m)
k
transform the input features into query and
key vectors, respectively. After that, a residual connection followed by Layer Normalization (Ba et al., 2016) is applied to
the output of the attention mechanism:
Ë†
hË†
(â„“,m)
i = LayerNorm 
h
(â„“âˆ’1,m)
i + hË†
(â„“,m)
i

. (16)
The normalized representation is then passed through a Feed-Forward Network (FNN):
Ë†Ë†
hË†
(â„“,m)
i = MLP2
â„“

ReLU 
MLP1
â„“

Ë†
hË†
(â„“,m)
i
 . (17)
Finally, a second residual connection and Layer Normalization are applied to produce the final node embedding for layer l:
h
(â„“,m)
i = LayerNorm 
Ë†
hË†
(â„“,m)
i +
Ë†Ë†
hË†
(â„“,m)
i

. (18)
C.3. Loss Functions
MoE Load Balancing. To prevent the issue of expert collapse (i.e., a small subset of experts dominates the processing
while others remain idle), we consider the MoE load balancing loss. Adopting the standard formulation proposed by Fedus
et al. (2022):
Lload = K Â·
X
K
k=1
Pkfk, (19)
where fk is the fraction of samples assigned to expert k, and Pk denotes the average routing probability for expert k across a
batch.
Structural Reconstruction. To preserve topological information, we minimize the binary cross-entropy loss over observed
edges E (positive samples) and randomly sampled unconnected pairs EË† (negative samples):
Ltopo =
X
mâˆˆâ„¦
"
âˆ’
1
|â„¦||E|
X
(i,j)âˆˆE
log 
Ïƒ

u
(m)
i
âŠ¤
Â· u
(m)
j
 âˆ’
1
|â„¦||E| Ë†
X
(i
â€²
,jâ€²)âˆˆEË†
log 
1 âˆ’ Ïƒ

u
(m)
i
â€²
âŠ¤
Â· u
(m)
j
â€²
 #
, (20)
where SRD is a structural projection head, Ïƒ is the sigmoid function, and u
(m)
i = DSR
m (h
(all,m)
i
), DSR
m is structural
reconstruction decoder for modality m, which is an MLP.
D. Proof of Theoretical Analysis
D.1. Proof of Theorem 3.2
In this section, we utilize the Information Bottleneck (IB) principle to prove that vanilla Multimodal Graph Encoders (e.g.,
MMGCN) inevitably discard synergistic features, while PLANETâ€™s topology-aware interaction preserves them.
Following the formulation in Graph Information Bottleneck (Wu et al., 2020), the goal of graph representation learning is to
maximize the IB Lagrangian LIB(Z) = I(Y ;Z) âˆ’ Î²I(X;Z), where Î² > 0 controls the compression trade-off.
Vanilla Multimodal Graph Encoder. The Vanilla Multimodal Graph Encoder processes modalities independently. For a
specific modality A, the encoding follows the Markov chain Y â†” G(A) â†’ ZA. The optimization objective is to maximize
the local IB: LV anilla = I(Y ;ZA) âˆ’ Î²I(G(A)
;ZA). To prove that this encoder discards synergistic features, We define
two potential encoding strategies:
â€¢ Drop Synergistic Features (DSF): The encoder captures only unique features, i.e., Z
âˆ’
A â‰ˆ {UA}.
14
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
â€¢ Keep Synergistic Features (KSF): The encoder captures both unique and synergistic features, i.e., Z
+
A â‰ˆ {UA, SA}.
We calculate the Marginal Contribution âˆ†LV anilla of shifting from Strategy DSF to Strategy KSF, which can be expressed
as âˆ†LV anilla = âˆ†IY âˆ’ Î² Â· âˆ†IG(A) . For âˆ†IY , we apply the Chain rule of mutual information (Federici et al., 2020):
âˆ†IY = I(Y ;Z
+
A ) âˆ’ I(Y ;Z
âˆ’
A )
= I(Y ;UA, SA) âˆ’ I(Y ;UA)
= I(Y ;UA) + I(Y ; SA | UA) âˆ’ I(Y ;UA)
= I(Y ; SA | UA).
(21)
Crucially, under the independent encoding setting, modality B is unobservable. According to Definition 3.1, the synergistic
feature SA contains no information about Y without the presence of SB (i.e., I(Y ; SA | UA) â‰ˆ 0). Therefore, we have
âˆ†IY â‰ˆ 0.
For âˆ†IG(A) , we similarly apply the Chain rule:
âˆ†IG(A) = I(G
(A)
;Z
+
A ) âˆ’ I(G
(A)
;Z
âˆ’
A )
= I(G
(A)
;UA, SA) âˆ’ I(G
(A)
;UA)
= I(G
(A)
; SA | UA)
= H(SA | UA) âˆ’ H(SA | G
(A)
, UA).
(22)
Since SA is intrinsically part of the input G(A)
, we have H(SA | G(A)
, UA) = 0, therefore:
âˆ†IG(A) = H(SA | UA) > 0. (23)
Substituting these results back to the Marginal Contribution âˆ†LV anilla:
âˆ†LV anilla â‰ˆ 0 âˆ’ Î² Â· H(SA | UA) < 0. (24)
Since the contribution is negative for any Î² > 0, maximizing the local IB objective necessitates the exclusion of synergistic
features to avoid incurring unnecessary compression costs. Consequently, we have Z
âˆ—
V anilla â‰ˆ {UA, UB}.
PLANET with the EDG Module. The topology-aware interaction mechanism aggregates semantic context from the crossmodal neighborhood. This implies that the encoding process follows a unified Markov chain Y â†” {G(A)
, G(B)} â†’ ZEDG.
The optimization objective is to maximize the local IB: LEDG = I(Y ;ZEDG) âˆ’ Î²I(G(A)
, G(B)
;ZEDG).
Similarly, we contrast two strategies: The first strategy captures only unique features, i.e., Z
âˆ’
EDG = {UA, UB}, and the
second strategy additionally captures the synergistic pair, i.e., Z
+
EDG = {UA, UB, SA, SB}. The marginal contribution of
transitioning from the first strategy to the second strategy is formulated as:
âˆ†LEDG = âˆ†IY âˆ’ Î² Â· âˆ†I{G(A),G(B)}
. (25)
Applying the chain rule yields âˆ†I{G(A),G(B)} = H(SA, SB | UA, UB) > 0 and âˆ†IY = I(Y ; SA, SB | UA, UB). Under
the joint view enabled by the unified Markov chain, the synergistic features become informative. By Definition 3.1, the
interaction information is strictly positive, yielding:
âˆ†IY = I(Y ; SA, SB | UA, UB) â‰« 0. (26)
Consequently, we have âˆ†LEDG = âˆ†IY âˆ’ Î² Â· H(SA, SB | UA, UB). Given that the significant synergistic information gain
outweighs the weighted compression cost (i.e., âˆ†IY > Î²Â·âˆ†I{G(A),G(B)}
), âˆ†LEDG becomes strictly positive. Consequently,
maximizing the joint IB objective necessitates the preservation of these features, leading to Z
âˆ—
EDG â‰ˆ {UA, UB, SA, SB}.
Finally, we quantify the information gap between the two representations:
I(Y ;Z
âˆ—
EDG) âˆ’ I(Y ;Z
âˆ—
V anilla) â‰ˆ I(Y ;UA, UB, SA, SB) âˆ’ I(Y ;UA, UB) = I(Y ; SA, SB | UA, UB) > 0. (27)
This theorem demostrates that our EDG module captures a strictly larger amount of relevant information than vanilla
Multimodal Graph Encoders which process MAGs as independent graphs.
15
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
D.2. Proof of Theorem 3.4
To analyze the generalization bound of our alignment strategy, we employ the 1-Wasserstein distance W1(Ë†Âµm, ÂµË†t) as the
metric for distributional discrepancy.
Utilizing the triangle inequality property of the Wasserstein metric, we decompose the total alignment error into three
components:
W1(Ë†Âµm, ÂµË†t) â‰¤ W1(Ë†Âµm, Î½Ë†m) + W1(Ë†Î½m, Î½Ë†t) + W1(Ë†Î½t, ÂµË†t). (28)
For W1(Ë†Âµm, Î½Ë†m). By the Kantorovich formulation of Optimal Transport (Villani et al., 2008), the Wasserstein distance is
defined as the infimum of transport costs over all valid joint couplings Î (Ë†Âµm, Î½Ë†m):
W1(Ë†Âµm, Î½Ë†m) = inf
Ï€âˆˆÎ (Ë†Âµm,Î½Ë†m)
E(x,y)âˆ¼Ï€[âˆ¥x âˆ’ yâˆ¥2]. (29)
We define Ï€
âˆ—
such that it transports the probability mass 1/N associated with each sample x
(m)
i
directly to its corresponding quantized token Q(x
(m)
i
). Formally, the joint distribution Ï€
âˆ—
is supported exclusively on the set of pairs
{(x
(m)
i
, Q(x
(m)
i
))}
N
i=1. Let C(Ï€) denote the transport cost associated with a coupling Ï€. Under our deterministic coupling
Ï€
âˆ—
, this cost is exactly:
C(Ï€
âˆ—
) = 1
N
X
N
i=1
âˆ¥x
(m)
i âˆ’ Q(x
(m)
i
)âˆ¥2 = Exâˆ¼ÂµË†mâˆ¥x âˆ’ Q(x)âˆ¥2. (30)
Therefore, we have the inequality:
W1(Ë†Âµm, Î½Ë†m) â‰¤ C(Ï€
âˆ—
) = Exâˆ¼ÂµË†mâˆ¥x âˆ’ Q(x)âˆ¥2. (31)
For W1(Ë†Î½t, ÂµË†t). Similarly, due to the symmetry of the Wasserstein metric and the same quantization mechanism applied to
the anchor text modality t, we have the corresponding upper bound:
W1(Ë†Î½t, ÂµË†t) = W1(Ë†Âµt, Î½Ë†t) â‰¤ Exâˆ¼ÂµË†t
âˆ¥x âˆ’ Q(x)âˆ¥2. (32)
For W1(Ë†Î½m, Î½Ë†t). We denote Î½
âˆ—
m and Î½
âˆ—
t
as the underlying ground-truth discrete distributions for modality m and the anchor
text modality t, both supported on S. Accordingly, the empirical push-forward measures Î½Ë†m and Î½Ë†t are viewed as samples
drawn from these corresponding ground-truth distributions. Leveraging the triangle inequality, we have:
W1(Ë†Î½m, Î½Ë†t) â‰¤ W1(Ë†Î½m, Î½âˆ—
m) + W1(Î½
âˆ—
m, Î½âˆ—
t
) + W1(Î½
âˆ—
t
, Î½Ë†t). (33)
Let Î´ âˆˆ (0, 1) be the scale parameter. According to Proposition 1 in Weed & Bach (2019), The 1-Wasserstein distance is
bounded by:
E[W1(Ë†Î½m, Î½âˆ—
m)] â‰² Î´
k
âˆ—
+
k
Xâˆ—
k=1
Î´
kâˆ’1 X
Qk
i âˆˆQk
E

|Î½Ë†m(Q
k
i
) âˆ’ Î½
âˆ—
m(Q
k
i
)|

, (34)
where k
âˆ—
is the truncation depth, and Qk
is the partition at scale k.
We simplify the bound by observing three key properties: â‘  For a sufficiently large truncation depth k
âˆ—
, the partition
resolution exceeds the minimum separation of DSRS vectors, thereby eliminating the truncation error (i.e., Î´
k
âˆ— â†’ 0). â‘¡
The finite support restricts the inner summation P
Qk
i âˆˆQk to at most C non-zero terms. â‘¢ The term E[|Î½Ë†m(Qk
i
) âˆ’ Î½
âˆ—
m(Qk
i
)|]
corresponds to the mean absolute deviation of empirical frequencies. By Jensenâ€™s inequality and the variance bound of the
binomial distribution, this is strictly bounded by q
1
4N =
1
2
âˆš
N
.
16
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Substituting these properties into Eq. (34):
E[W1(Ë†Î½m, Î½âˆ—
m)] â‰¤
Xâˆ
k=1
Î´
kâˆ’1 X
Qk
i âˆˆQk
E

|Î½Ë†m(Q
k
i
) âˆ’ Î½
âˆ—
m(Q
k
i
)|

â‰¤
Xâˆ
k=1
Î´
kâˆ’1
Â·
ï£«
ï£­
X
C
j=1
1
2
âˆš
N
ï£¶
ï£¸
=
 Xâˆ
k=1
Î´
kâˆ’1
!
Â·
C
2
âˆš
N
.
(35)
Since Î´ âˆˆ (0, 1), the geometric series converges to a constant CÎ´, yielding E[W1(Ë†Î½m, Î½âˆ—
m)] â‰¤ CÎ´ Â· âˆš
C
N
= O

âˆš
C
N

. Both
W1(Ë†Î½m, Î½âˆ—
m) and W1(Î½
âˆ—
t
, Î½Ë†t) satisfy this bound, the total discrete alignment error converges at the rate:
E[W1(Ë†Î½m, Î½Ë†t)] â‰¤ E[W1(Ë†Î½m, Î½âˆ—
m)] + E[W1(Î½
âˆ—
m, Î½âˆ—
t
)] + E[W1(Î½
âˆ—
t
, Î½Ë†t)] â‰¤ E[W1(Î½
âˆ—
m, Î½âˆ—
t
)] + O

C
âˆš
N

(36)
Substituting Eq. (32), (36), (31) into Eq. (28), we obtain the final bound:
W1(Ë†Âµm, ÂµË†t) â‰¤ Exâˆ¼ÂµË†mâˆ¥x âˆ’ Q(x)âˆ¥2 + Ezâˆ¼ÂµË†t
âˆ¥z âˆ’ Q(z)âˆ¥2 + W1(Î½
âˆ—
m, Î½âˆ—
t
) + O

C
âˆš
N

. (37)
Comparison. Consider the standard case in continuous space R
d
. According to Theorem 1 and Proposition 7 in Weed &
Bach (2019), the convergence rate of Wasserstein estimation is governed by the lower Wasserstein dimension dâˆ—(Âµ) of the
measure. For continuous feature distributions in high-dimensional spaces (e.g., dense semantic embeddings in R
d
), the
intrinsic dimension dâˆ—(Âµ) typically equals the ambient dimension d. Consequently, we have:
E[W1(Ë†Âµ, Âµ)] â‰³ N
âˆ’1/d
. (38)
Conclusion. Eq. (38) implies that for high-dimensional features, the convergence rate is extremely slow. In contrast,
by leveraging the DSRS where modality alignment is explicitly constrained by the General Knowledge Loss, PLANET
accelerates the convergence rate to O(C/âˆš
N). Simultaneously, Minimizing LV Q in Eq. (6) effectively minimizes the upper
bounds of W1(Ë†Âµm, Î½Ë†m) and W1(Ë†Î½t, ÂµË†t), ensuring that continuous features lie close to DSRS. These mechanisms achieve
effective modality alignment.
E. Experiment Settings
E.1. Supervised Learning
Baseline Settings. For supervised models (i.e., GCN, MMGCN, MGAT), we train them directly on downstream datasets
without any pre-training. In contrast, for self-supervised and graph foundation models, we follow the pre-training and
fine-tuning paradigm: models are first pre-trained across various datasets and subsequently fine-tuned on each specific
downstream task.
During the fine-tuning stage, for baselines that do not provide specific methods for node classification or link prediction,
we add an MLP to perform these tasks. Specifically, the input embeddings of MLP are encoded by the pretrained graph
encoder. For node classification, we set the output dimension of the MLP to the number of classes. For link prediction, we
concatenate the embeddings of the target node pair as input to the MLP, where the output dimension is 1, reflecting the score
for predicting the pair as a positive sample.
Feature Encoding. Both raw text and image data are encoded using Qwen2-VL-7B-Instruct (Wang et al., 2024a) to generate
high-quality initial node embeddings. For baselines that are incapable of processing MAGs (e.g., GraphMAE2, GFT), the
embeddings from modality-specific encoders are concatenated along the feature dimension. It is worth noting that while
RiemannGFM and SAMGPT prioritize the learning of topological knowledge, they use node features in specific ways (e.g.,
17
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 6. Detailed parameter settings for few-shot node classification and link classification tasks.
Task Parameter Value
Few-shot Node Classification
n train 20
n query 10
n task 10
Few-shot Link Classification
n train 20
n query 40
n task 10
RiemannGFM uses node features in downstream tasks). In our implementation, we replace these node features with the
representations encoded by the modality-specific encoders, which have significantly richer semantic information.
Pre-training Datasets and Sampling Strategy. Self-supervised and foundation models are pre-trained in same weights of
datasets (weights are presented in Table 5). For baselines that are incapable of taking multiple datasets as input or cannot
scale to large-scale graphs, we implement a sampling approach. Specifically, for each node in the graph, we extract its
2-hop neighbor subgraph. We then randomly sample a fixed number of these subgraphs from each dataset according to the
pre-defined weights (Table 5), forming the training batches. This guarantees that all models are pre-trained on an identical
data distribution.
E.2. Few-Shot Learning
Overall Settings. Most of our experimental settings are the same as those in Appendix E.1 (e.g., feature encoding, sampling
strategies.) Following Wang et al. (2024b), we adapt the â€Pre-training and Fine-tuningâ€ paradigm. However, during
the fine-tuning stage, we do not use an MLP as the classification head. Instead, we use a prototype-based method for
classification. The specific values for the parameters used in our experiments are detailed in Table 6.
Few-Shot Node Classification. For an N-way K-shot task, we randomly sample n train samples for each class from
the training set. On the test set, we randomly select N classes and sample K instances per class to serve as prototype
vectors, along with n query instances for evaluation. These N Â· K samples are used to construct a prototype classifier,
where the embedding of each class is computed by averaging its corresponding K sample embeddings, resulting in N class
embeddings For evaluation, we calculate the cosine similarity between the evaluation vector and each class embedding,
assigning the sample to the class with the highest similarity. The same procedure applies to the validation set. To eliminate
randomness, this sampling process is repeated n task times, with the final result being the average of these runs.
Few-Shot Link Classification. Since the original datasets (i.e., Amazon-Sports, Amazon-Cloth) are designed for link
prediction tasks, we adapt them for link classification tasks. Specifically, we aggregate all positive edges from the training,
validation, and test sets to form positive samples. We simultaneously generate an equivalent number of random negative
edges to serve as negative samples. These negative edges are allocated to the training, validation, and test sets to strictly
match the count of positive edges in each respective set (e.g., a test set containing 500 positive edges is assigned 500 negative
edges). Consequently, we formulate the problem as a balanced 2-way K-shot binary classification task. The subsequent
method follows the Few-shot Node Classification described above.
E.3. Multimodal Generative Tasks
G2Text. We evaluate the performance of G2Text generation on two multimodal datasets:
â€¢ Grocery. This is a real-world e-commerce dataset. The objective is to generate a comprehensive product description
for a target node. The input consists of the target nodeâ€™s product title and the multimodal context (images and texts)
from its neighbors (including target nodeâ€™s image).
â€¢ Flickr30k. Originally an image-text retrieval dataset, we adapt it for graph-based generation by constructing a k-NN
graph based on feature similarity. Each node contains an image and five distinct textual descriptions. For the G2Text
task, we utilize the last four descriptions of the target node and the multimodal information of neighbor nodes to
generate the first description.
18
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 7. Specific prompts for Grocery and Flickr30k datasets in G2Text task.
Dataset Prompt
Grocery
### Task: Generate a natural-language description of the product node.
### Input: Title: <item title of the target node >.
### Output Results:
Flickr30k
### Task: Generate a detailed description for the image based on the context.
### Input: Context: <desc 2 >, <desc 3 >, <desc 4 >, <desc 5 >.
### Output Results:
For the specific prompts used for each dataset, please refer to Table 7.
G2Image. We evaluate the G2Image task on two datasets: Goodreads-NC and Ele-fashion. Following the experimental
setting of InstructG2I (Jin et al., 2024), we focus on specific, visually distinct categories to verify whether our embeddings
capture sufficient semantic nuances to drive fine-grained generation. For Goodreads-NC, we select nodes belonging to
the history and children categories for training and testing. For Ele-fashion, we select nodes from the jewelry and shoes
categories. Distinct from the original data partition used in InstructG2I, we adopt a standard randomized partition strategy
for our experiments. Specifically, for each selected category subset, we split the data into a training set (80%) and a testing
set (20%). This ratio ensures that the model is trained on sufficient data while reserving a substantial portion for robust
evaluation.
F. Implementation Notes
Detailed hyper-parameter settings for the pre-training stage are summarized in Table 8. We optimize the entire framework
using AdamW optimizer for 5 epochs.
19
Toward Effective Multimodal Graph Foundation Model: A Divide-and-Conquer Based Approach
Table 8. Detailed hyper-parameter settings for pre-training.
Category Parameter Value
Optimization
learning rate 4e âˆ’ 5
weight decay 5e âˆ’ 4
batch size 128
epochs 5
optimizer AdamW
EDG Module
num experts 5
top-k 2
num layers 8
num heads 8
NDR Module
DSRS size 20480
DSRS dim 768
DSRS temperature 0.93
Loss
Î²1 (feature reconstruction) 0.1
Î²2 (structure reconstruction) 0.1
Î²3 (general knowledge) 0.2
Î²4 (VQ) 0.1
Î²5 (MoE load balance) 0.01
Î²inter 0.5
Other Params
hidden dim (d) 768
dropout 0.1
select node mask p 0.6
select modality mask p 0.4
edge mask p 0.15
20