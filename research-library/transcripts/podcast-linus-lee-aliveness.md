<!-- research-library/transcripts/podcast-linus-lee-aliveness.md -->

Linus Lee: Engineering for Aliveness - LLMs, Agency, Tools for Thought, Thrive Capital |Dialectic 24

https://www.youtube.com/watch?v=IaUYbNnOYUM

Welcome to Dialectic episode 24 with Linus Lee. Linus works in the world of software building, researching, engineering, designing And exploring how technology and software can amplify us rather than diminish us. he hopes to create what he calls instruments for super agency. Leaning into the notion that technology at its best should make everyone more human and more capable. These days he's focused on ai. At Thrive Capital, a VC firm where he builds internal tools, researches and advises. Before Thrive, Linus worked at Notion Betaworks and Replit, across engineering, research and ai. and he's also prolific in his own time With over a hundred personal software side projects and extensive writing, much of which is incredible. One of the most foundational things he explores in his work is How language and knowledge can be codified, explored, and expanded by way of software. we talk about all of this in more. Including one of my favorite framings he has where he distinguishes between what he calls instrumental and engaged interfaces and why some tools just need to get the job done while others help us more deeply understand and move toward mastery. Running through the conversation, as you might guess, is the state of LLMs and how they affect software. And we also talk extensively about his work at Thrive and how he thinks about bringing an engineering mindset there. I hope you notice it throughout, but we also end the conversation specifically talking about how he, humanity and technology don't need to be at odds and in fact, how technology can help us dream and help us wonder. I hope you enjoy the conversation and are as inspired as I was by Linus If you enjoy this episode and you enjoy dialectic, please share it with a friend. Every little bit counts, and it means the world to me. With that, here's Linus. Linus Lee, good to see you. Thank you for having me. I'm excited. It's exciting. I like to interview multifaceted people, and I also like to interview writers. I sometimes joke that the ideal interview is with somebody who has eight really good essays. You have a lot more than eight, and you also have so many projects. We're going to cover some things today, but not everything. I probably do have seven or eight really good ones. But in the process, I've also created a lot of exhaust that is not perfect, but it's still fun. It's part of the process. I want to start with a quote from you, something you wrote in one of the tidbits in your stream—not even a tweet or a post. You were talking about conferences, and you say, "When I write a talk, I almost always just want you to walk away thinking about the technology you create as an instrument for advancing your values and a lens through which to view the world with those values." As I mentioned, you are wildly prolific—polymathic creatively, technically, and professionally. But it's clear that values are underpinning almost everything you do. One of those values you care a lot about is agency. Agency is a popular topic, especially on Twitter recently. The way you approach it is a meta view that technology can be power-consolidating or widely empowering. You have a frame in particular that I really like. My question is, how does technology extend or extinguish agency? Linus There are two big ideas, and I think this will be an overarching theme that comes up over and over in our conversation. There's a broader idea and then a narrower, personal idea. The broader idea is that if you're building technology, it is such an amplifier of your personal work and impact in the world that it's good to be thoughtful about the impact the technology is going to have. It’s not just in terms of the economic impact or anything concrete, but anything that you put out into the world is going to push the world in some direction. You should make sure that, first, you're intentional about that direction. Second, that you're thoughtful about building technology from knowing what direction you want to push the world in, rather than deriving your ideology as a function of what you end up stumbling into building. Regardless of my personal values, my push to anyone building technology is to be thoughtful that any direction isn't just forward; it's also in a specific, opinionated direction. With that said, for me personally, agency as a concept in the recent past—at least for people who are very online—means something very specific, like you can just do things. I take a more platonic and slightly higher-level view, which is if you have a particular point of view, having more agency means you can more freely do the things that push the world in a way that's a reflection of you and what you want to see. Technology is definitionally an agency amplifier, but by default, it tends to amplify people's agencies in a biased way. One way to view technology is as a way to turn capital into impact. Capital can be human capital, money, or other resources. The shape of any technology is : I have a resource—money, people, or a team—and I can turn that more efficiently into a desired result in the world. Because of that, technology by default has a tendency to give more impact or concentrate more impact towards the people who already have a lot of resources. But we can't stop building technology, because technology is also the reason for so many good things and so much progress in the world. There's a way to build technology in a more opinionated way, where you layer on your own opinion about what it should look like and how it should be usable to counterbalance that default posture. Instead, it can be designed and distributed in a way that helps individual people equitably and in an egalitarian way distribute their impact in the world, rather than just following the default grain of technology writ large, which has a tendency to concentrate power. There's a piece of that, too. "Equal" is a challenging word, but at the very least, the accessibility of it should be widely distributed. That it be widely distributed feels like a really important part. There's a lot of discussion, especially regarding AI, of equitably distributing the impact. But it's also really important that the capability for technology to amplify people's will and values also be, to the extent that anything can be, equitably distributed. You have a frame on this : the phrase ‘instruments of super agency,’ which neatly sits next to the superintelligence idea. We talk a lot about superintelligence. What do you mean by that? I really like phrases that have a lot of loaded connotation, and this is one of those that is really high density. There's "instruments," and then there's "super agency." Super agency may be a little bit more straightforward. Superintelligence is weird because it implies there is some finite level of normal intelligence, and then there's superintelligence, which is the level of intelligence that exceeds it. That's a problematic point of view. But to the extent that you subscribe to that way of looking at intelligence, you can look at super agency in a similar way. There's a normal amount of agency that an ordinary human has in the world by personally interacting with other people, organizations, or the physical world. Super agency would be the concept of giving that person wider impact, having that person be able to push the world more in whatever direction that they desire with less effort, much farther. I just generally love "instruments" as a word. I particularly love it because it conjures this image of something that is quite intricate, has a lot of depth, and requires substantial practice to gain mastery. Instead of writing three lines of code or spending five minutes downloading something to master it, an instrument like a violin can take a lifetime to master. It can take centuries to master. There's a cost. There's a cost. Another way to describe it is that you have to grow into it. You can get some benefit by spending three or six months on it, but there are so many deepening layers of benefit the more time you put into it. You can't finish a violin in the same way that you can finish a video game. There's always more to learn. I think really great, engaging tools have that trait where there's always more to get out of the tool and more depth that you can find. I like that a lot. You write extensively about the idea of instrumental versus engaged interfaces in the context of tools, interfaces, and technology. When we first met, one of the first things we talked about was your metaphor of maps versus GPS navigation. The difference can be subtle. They have a similar goal, but they use fundamentally different approaches. It would be helpful to have you explain instrumental versus engaged, and then specifically talk about why some amount of friction can actually be good for enabling agency in a tool. We've talked a lot at a conceptual level, and this will take me down to a more personal level. When I first started thinking about tools, I got into it because I was a huge productivity nerd, probably like a lot of you listening. I would try every new to-do list tool and every new note-taking tool. There was a time in the tech industry when note-taking tools were very hot and it was the sexy thing to work on. I got into thinking about tools that way and amplifying agency that way. I think a lot of people who love thinking about this kind of stuff have an intuitive bias that working for something, and needing to work to get something in return, is inherently a good thing. They want to see more detail. They want to be power users. Maybe I'm grossly oversimplifying, but a lot of people, including myself, that really like thinking about these tools want to be power users and think that the world would be better if there were more power users of all these tools. I spent a few years building tools with that assumption. One of the ways in which I've grown as a person who thinks about building tools is something Ivan at Notion once said in a company all-hands. He said Notion is all about democratizing the ability to build tools for yourself, which is such a "me" idea. I love that. But then he also said that normal people don't wake up in the morning wanting to build software. They just want to have fun, help somebody, or solve some problem in their life. Building software is this incidental thing that they sometimes have to do to make that a little bit easier. One of the ways I've grown concretely is realizing that for most people—and even for me—there are certain things in my life where I really don't care how it's done. I don't care to interface deeply with the texture of the task. I just want something delivered to my home. I want to get in a ride-share and end up somewhere. I just want the result. The taxonomy that I use when thinking about building tools these days is that for certain people and certain goals, the way you want to get there is just by describing what you want and then getting the result. You want to do that as cheaply, quickly, effortlessly, predictably, and reliably as possible. That is an instrumental point of view : tools are there to take a specification of a goal and deliver the result as quickly, cheaply, and reliably as possible. But then there is this other kind of tool whose job is to get you as deeply engaged with whatever you're doing. Musical instruments are a great example of this. Maps are a great example of this, which we'll talk about in a moment. Lots of crafty tools like IDEs are another great example of this. Their job is to put you face-to-face with all of the requisite complexity of whatever you're dealing with. The complexity is actually good in that case, because whatever you're doing requires you to contend with that complexity. If you were trying to perform a sonata, it would suck if you had to just press a button and then listen to whatever was generated. You actually want to perform to your fullest with as much nuance and detail as you can put into this thing. It's great to have an instrument that lets you express yourself. Very concretely, if you're buying a more expensive piano, one of the reasons that better pianos can be better is that they let you express more deeply. But then you also have to think more about what you're doing. More mastery required. More mastery is required. It's not a Guitar Hero thing where you do the thing and there's a ceiling to how good you can be at it. Maps and GPS are the most canonical example of this. Sometimes GPS is totally the right thing. If you want to get in a car and end up somewhere because you're in a rush, GPS is great. A self-driving car is even better. But sometimes… I went to Catalina Island recently on a very spur-of-the-moment trip. I went into a tour guide station and bought a physical map. I had a phone, but I bought the physical map because it had some hiking trails, and I used that map to run around. It was fun to learn about the physical space around me by actually working with this physical object. The point of me being at the island was to roam around and learn about what was there, not to get somewhere. In those cases—when you're composing music or sometimes when you're writing programs, which maybe we should also talk about—there are lots of areas and different domains where it's good for the tool to force you to contend with the complexity. I call those engaging interfaces or engaging tools. A common fallacy I've fallen into is to say that some people want instrumental tools and some people want engagement—that there are these power… Users versus non-power users. Some people are lazy and some people aren't lazy. I've fallen into that fallacy at times. I've also fallen into another fallacy, which is to say certain tasks require mastery and certain tasks don't. That is also not true. It's really a function of all of these things. For some people at some moments, you just want the result. For the same people in other moments and other tasks, they actually do want the mastery and the complexity. It's very situation-dependent. Even if you're building a calendar tool, a productivity tool, an IDE, or what have you, it depends on who you're selling to and why that person is trying to use that tool. Sometimes the right thing to do is to make it as cheap and easy to get the results as possible. Other times, revealing the full complexity of the medium is the right thing. It feels like we're moving towards something I actually talked about with Jeffrey Litt on the podcast. This is a quote from you : "The ideal instrumental interface for any task or problem is a magic button that can one, read the user's mind perfectly to understand the desired task, and two, perform it instantly and completely to desired specifications." You might call that an agent, which is obviously top of mind with regard to LLMs. We'll talk more about the specifics later, but from a super broad standpoint, it seems we're on a trajectory towards all utility needs being instrumental over time. I'm curious for you to challenge that. It seems that, at the very least, the trajectory is going that way. To go back to your simplest example, if I'm just trying to get somewhere, I don't need a physical map. I probably don't even need turn-by-turn navigation; I just want Waymo. If I'm on a vacation and trying to hang out and have fun, I'll take your complexity. Maybe that's an oversimplification, but I'm curious what you think. I agree with your intuition that this kind of thing that people are building, which we've decided to call agents, is trending in the direction of the perfect instrumental interface that I talk about. I could imagine a future where this thing can read your mind and perfectly execute whatever you want. Before you even realize you've thought it. The kinds of things that engineers build are partly a function of what they think should exist, but there are also lots of other factors that influence what people build. Other things that influence what people build include if it is cool to build this type of thing and if it is easy. I'm concerned that because agents feel very sexy and novel right now—and are in some ways the easy way out to build a new type of powerful thing—problems that may be better solved by forcing the user to contend with the complexity of something are going to instead be solved by these instrumental tools that take away the user's agency. One example of this is software. There are a lot of agentic coding tools around, and there are ways to intentionally use them that are really powerful. On my team at Thrive right now, our designer has been using a lot of these coding agent tools, and it's been really fantastic. Not just that we can build more, but we can build better things that look better, feel better, and work better. So there are great ways to use it. But because agents are the sexy, maybe the creatively easier thing to build right now, I think too many people are using agents too much of the time. They're building software systems that they don't fully understand or they're not pushed to fully understand. I think that'll probably come back to bite those people later. We'll talk more about the merits of the instrumental side later, but I want to talk about the engaged part and specifically what you call technology representations. You have a definition of engaged interfaces, or at least a description of them, oriented around two ideas : seeing and expressing. You say a good engaged interface lets us do two things. One, it lets us see information clearly from the right perspectives, and two, express our intent as naturally and precisely as we desire. To see and express is what creative and exploratory tools are all about, alluding to your earlier point that creative and exploratory tools are different than purely need-based ones. Maps are a great example of this. You also say a representation must abstract. What you're pointing to here leads to my question : Why can highly accurate faithfulness to reality be a bug, and conversely, lossiness be a feature? Put another way, does complexity always reduce agency? Maybe those are two separate questions. Good, engaged interfaces have two jobs : to help you see what's happening and help you express your intent as fluidly as possible. This is one of my favorite sentences I've ever come up with because I remember having a bit of a mini existential crisis. I was in Berlin at the time and had just come back from a conference where I had talked about tool building, and I got stuck in a conceptual rut. I work on interfaces and talk about them all the time, but what is a good interface? How do I know? How would you grade how good an interface is? I started melting at the realization that I talk about how to make interfaces good, but I actually don't know what makes them good. I needed to figure out my definition of "good." At the time, I didn't have this "instrumental" and "engaged" vocabulary. But in current terms, what I came up with was this : see and explore. Good technologies and good interfaces let you very clearly see what's happening; they don't obscure things that don't need to be obscured. Then they let you take action on it, which is the mechanism by which your agency impacts the world. You can express your intent towards the level of precision that is needed for you to have the desired level of agency. I really love that phrasing. An inherent part of that is you can't see everything. If you're looking at a map, a diagram of anatomy, a chart, or even a code base through a design, you can't see every little detail. In that way, everything is a map. The map not being the territory is such a feat. You have this amazing excerpt where you're talking about the empire that built an empire-sized map. It's a perfect one-to-one scale, and it's so useless. That is a Borges short story. It's about a group of scholars that decide to build a map, and they desire to build more and more accurate versions of maps. The map has to get larger and larger until it is exactly the size of the territory and covers every inch of it. Then the map is totally useless—a vapid exercise in scientific accuracy. Which brings us to abstraction. Abstraction is necessary for agency. It feels intuitive, but if I had to articulate why, it would be that if you had a map that was the size of the United States, you would not be able to carry that map or look at it to go somewhere. It would just be unwieldy. More concretely, the point of an abstraction is to give you a model that you can fit in your head and work with. It's almost compression. If I want to understand the transformer, or some part of the economy or a company, the point of having a model or an abstraction is that I can fit it in my head and do things with it to figure out how to understand it or what action I need to take to push that reality in a direction I want. If there were infinite levels of detail, the point of having a model would be lost, because then I could just mess around with things in the real world, but it would take forever to understand every little detail. This is true of all scientific models, abstraction generally, and also in the software sense. Good abstractions necessarily lose some detail, and there's some room for opinion here. If you go to Google Maps or Apple Maps, there's a bunch of different options of which abstractions you want to use your map with. You can do the terrain, the transit map, or the roadmap. These are different models of the real world. None of them reflect reality fully, but they're useful for different types of things. You're also not likely turning them all on at once, correct? That would also be a very hard map to use. This leads into the juxtaposition between constraints and abstraction versus more complexity, and how balancing between those two things can give or take away agency. You've said we need diverse and accessible representations, but then you've also talked about introducing complexity as a way to take the user seriously. With a musical instrument, the more complexity, the more you can do. I love that. Take the user seriously. Forcing the user to contend with complexity, in your words. You also said, on the note of constraint, that you can reduce the number of choices the user has and contextualize the input UI to shape their behavior. That's me paraphrasing you on interfaces. As a last example, an excerpt from you about video games captures this tension. You say a video game, for example, can sometimes be better by being more realistic and easier to learn. But this isn't always true. Sometimes the fun of a game comes from the challenge of learning its mechanics or strange, surrealist laws of physics in the game world. A digital illustration tool is usually better off giving users more precise controls. But there are creative tools that lead artists to discover surprising results by adding uncertainty or elements of surprise. This is a needlessly challenging question, but can you square the circle on this? How do you think about when we actually want to add complexity and friction versus when we want to abstract and make things simpler for someone? This is a hard question. First, I would go back to how we opened and say some part of this is about being a creator of a tool. Let's say you're building a drawing tool, a tool for making images. As the designer of that tool, you may have some aesthetic that you want to proliferate into the world. You could say, "I think the world would be more interesting if we pushed artists to create with imprecise tools or take into account more serendipity." This is an opinion you could express through the vessel of the tool that you're making. It's almost a prompt, like a writing prompt or any kind of prompt. And it can be really effective for helping someone get creatively started. Yes, exactly. This is a reflection of your values or your aesthetic. Ideally, you always make things as a reflection of your aesthetic or your values. Other times, you may want to give people as much power as possible, and then the precision and the reflection of reality may be useful. That's one way of looking at it. Another way of looking at it may be that sometimes you're building instrumental tools and you want to help the user elicit what they want out of themselves as easily as possible. For different tasks, there are also different correct levels of abstraction. If you are trying to research a company for a high school research report, you need a different level of abstraction than if you're trying to do a leveraged buyout. Even on the more engaged, more detailed, complex side, there is still room for tool builders to express their preferences or values. One interesting avenue of this is that there are a handful of really interesting tools out there that try to help researchers understand extant literature about a topic in medicine and machine learning. They stretch a gamut. Some are more for power users, and some are more, "Tell me the question and I'll deliver the result." Whether intentionally or not, all of those are an expression of some value the creator of the tool has about what the right level of detail is for the user of this tool to have to contend with. A tool may give you the answer, or it may say, "Here are 20 papers that agree with you and 10 papers that don't. Now deal with this." Depending on who you are, you may want to force the researcher to deal with this conflict in the literature. The fact that this spectrum exists gives people who build tools room to express what they think is the correct level for that particular task for that particular person. It also seems that the instrumental-engaged idea is a gradient in and of itself. Maybe as tools become more adaptable over time, it’s like a good video game. There's a notion I really love about games from CT Nguyen, where he talks about video game designers sculpting agency. If a game is too easy or too hard, it's almost adapting with you, which I think is important. There's a related rant that I've given to some of my friends recently. With any given level of technology, there is a frontier. If you want to build something that is easy to use, you need to sacrifice some level of complexity. And if you want to make something really accurate and detailed, you may need to sacrifice on learnability or usability. So there is this frontier, but this frontier changes over time, and this frontier itself is a thing that you can move. I'll give you a concrete example. I was talking about coding AI with a friend recently. One kind of advancement that you can make is to build systems that are better at using current technology to build software. I went on this thought experiment where I said, let's imagine that we're in an alternate timeline where we have built super-intelligent, super-capable code generation systems, but the best programming language technology that was available was C. You have this super coding AI, and you can tell it, "Please build Google Chrome," and it'll write a perfect C program in one shot that is Google Chrome. In this world, would you still want to invent Python? Would Python still be useful? For a normal person going around their day not building software, it does not matter. But if you're a person whose job is to think about what software to build and how to build robust, resilient software systems, then Python is a huge advancement. At the same level of software complexity, it lets you build it much more easily. Conversely, at the same level of ease of use, it lets you handle much more complexity. It's a notational innovation or an innovation in how we represent and model software. It's an innovation in abstraction that pushes this frontier tradeoff between simplicity and complexity forward. It goes back to the seeing and exploring thing. In Python, you can see much better what the system is doing and how it's composed, and you can express your intent much more fluidly without having to worry about things like pointers. I love that example. It's a little zoomed out, but it feels not that far from the notion of : if we could communicate telepathically, would we have needed to invent language? The first example that comes to mind is a sci-fi trilogy called _The Three-Body Problem_. It's a slight spoiler, but the aliens we find out can't really lie because they have no difference between thinking and talking. It's really interesting to think about the many ways we shape our tools and our tools shape us. Language and abstraction are core tools. The next thing I want to talk about is thinking tools, and I'll start with a few quotes from you. You say, "Despite its ubiquity, the most interesting and important part of creative knowledge work—the understanding, coming up with ideas and exploring options part—still mostly takes place in our minds, with paper and screens serving as scratch pads and memory. More than true thinking aids, there are very few direct manipulation interfaces to ideas and thoughts themselves, except in specific constrained domains like programming, finance and statistics, where mathematical statements can be neatly reified into UI elements." Then you go on to say, "In the best thinking tools today, we still can't play with thoughts, only words." Finally, "While building tools to solve hard problems for humans, we should strive to also improve people's depth of engagement with those complex problems and their solutions as a way to preserve human agency when working with increasingly capable aids for our work. Otherwise we risk losing touch with and therefore understanding over critical decisions." Tools for thought is a can of worms, but it's an area you've spent a lot of time on and one that many curious people are very interested in. It ties nicely back to our conversation about representations. Can you talk about why representations for thinking—or maybe just notation, as we were just discussing—is such a compelling dream? One thing I'm personally afraid of as we build more technology is that as we build stuff that absolves us of our need to really understand what's going on, we are going to be pushed to not care about understanding. This comes back to this instrumental versus engaged idea. For all critical systems in the world—things related to money, health, and how we govern ourselves—we should have really smart people who deeply understand and try to advance the frontier of our understanding of how these systems work. A core belief of mine, maybe just based on intuition, is that agency, or the ability to understand and influence what's going on, is really important. To have that level of agency, we need to also have a full and detailed understanding of how these systems work. If you want to write economic policy, you need to have a really deep understanding of the economy as a machine. If you want to write software, you better have a really deep understanding of how computers work at a mechanical level. I don't want to be intermediated or automated out of understanding. More concretely, this dream of a UI—I have a blog post called "Runtime for Structured Thought" or something similar—came out of building a note-taking app. I had this idea that I could not let go of : I dump all these notes into my notes app, but in the process of me actually writing these thoughts down, all the thinking is still just happening in my head. If you're doing long division, there is some part of the long division thinking process that is taking place on the piece of paper. There is some mechanical thing that's happening with your paper and pen that is not a thing that's happening in your head. But it's almost between your head and the paper. It's between your head and paper. We've invented a way to externalize a kind of thinking into writing things down. Programming can sometimes feel this way. You've found a way to mechanically externalize something that used to happen in your head onto paper. I really like the feeling of that; it's so cool. Writing can do that at a slightly slower latency. It captures that, but it's more like looking at what you've already written. Writing is different than thinking; there's a lag. The cool thing about long division is that it's more obvious when you've done something wrong. There's a scaffold. There are certain kinds of invalid things you just can't do when writing down long division or doing math on paper. The same is true with certain kinds of algebra, but this is restricted to math. I wanted some way to think out loud on paper where if I wrote something that was logically incorrect, I physically would not be able to write it down. That would be so cool to have. A little devil or angel on your shoulder. The way I want this to happen is not that I would write two sentences down and then some chatbot on the computer says, "Hey, I think these are incompatible clippies." No thanks. What I want is to write these things down. It would be so cool if, as I write my second sentence, I run out of room or it's geometrically not possible for me to continue writing the sentence because it's incorrect and not compatible. I want this feeling of fitting puzzle pieces together for thinking out loud on paper, where the way you write things down mechanically helps you think better. That's an aesthetic thing that I want to proliferate in the world, and that's ultimately what I'm pursuing. The reason I got into researching embeddings is because it felt like the closest thing we have to this dream of turning conceptual compatibility, incompatibility, and coherence into geometric things that can exist in the real world. When I was deep into latent space in 2022, I would tell everyone to imagine that in the future, the way you read a book is not by opening it. Instead, you walk into a huge room where against the walls are sculptures. Each sculpture is a big idea. Different ideas have different shapes that you can tell from a distance. As you get closer, you see more detail in the shapes, and that detail corresponds to the details in the claims. Maybe you have your own sculpture that corresponds to your belief in the world. It would be visually obvious where they're incompatible because they look like things that wouldn't fit together as puzzle pieces. This is a reification of something structural about thoughts, not just the words. How can you express it visually? It felt like embeddings could capture that, where similar ideas are physically closer together and maybe you could turn them into shapes. There's research I want to continue doing on turning the meaning of ideas into shapes. Thinking should be like putting puzzle pieces together. It's pointing at a notion I believe, which is that we are fairly under-explored when it comes to spatial interfaces. There might be a lot of heavy lifting that AI, broadly, might be able to do to help us better investigate them. Other things are going to have to happen, and maybe you need a VR experience. It almost feels like those two categories of technology might feed off each other over time. Your example makes me think of something that didn't require computing technology for people to come up with, like a memory palace. It felt like we stalled out on really pushing the boundaries of spatial interfaces for a little while. I wonder if that's something we'll return to in the near to medium term if you can just generate a world really quickly based on the embedding of a book. Absolutely. The way that humans interact with information of all kinds has been taken over by writing. I'm not very deep in the pure math world, but an interesting discussion tidbit I heard was that there are certain kinds of proofs you can express as a diagram. There are certain kinds of statements about the geometry of triangles and angles where, if you draw a diagram, anyone can look at it and it's so obvious that this is the case. But to prove it in writing requires a lot of mechanics. There's an interesting discussion about different representations. Some are more efficient at certain kinds of claims. A lot of the culture of pure mathematics, as I understand it, is predicated on written proofs and prose. It's a particular written tradition that's biased against this other way of arriving at conclusions. In general, we are so used to the conflation of rigor of thought and writing lines of text on paper. But that doesn't have to be what thinking and communication feel like. The way this rubs me wrong—turning everything into reading and writing text—is the same kind of frustration I feel about everything turning into chat. Some things are fine as chat, and some things are best expressed by writing them down. But in writing, in books, and in the way thinking has come to look, so much of it is just reading a bunch of text, loading all this complicated state into your brain, doing a bunch of abstract, amorphous stuff, and then writing more ideas out. If you had to turn that into the shape of a software interface, that is what chatbots are. Instead, I want to work with things like charts, tables, plots, and diagrams that are totally other, still very rigorous ways of expressing and working with ideas. Or even these sculptures and things that are more direct and feel more like they exist in the universe of things the human body is good at working with. It feels that we're nearing the end of the written tradition being the dominant way media is consumed and created. We're moving to something that's more oral, like audio and video, but it's experienced through a two-dimensional screen, so there's less spatial interaction. Simultaneously, as you mentioned, large language models are showing up at the tail end of the text world, grounding us back in text. It feels that all of that is very flat, which is interesting. I'll add one more bit to that, which is about building technology that's a reflection of how you want the world to look. Even in the history of the written tradition, technology has further and further constrained what writing looks like. In the beginning, we had handwriting. For a long time, we had handwriting, and then we had more structured handwriting—writing manually, copying down the Bible, but you had to put things in lines and there were guardrails around the paper. Still, it was handwriting. If you made a mistake, you just wrote over it. Then we had typewriters, typesetting, and now digital documents. If you're writing an HTML document, it's really hard to make stuff that's free-form. I went to British National Library, where they have an exhibit of old books. The really cool thing about these books is that they were so expensive to make that they were heirlooms. It's almost like a painting. Exactly. These were also fully handwritten, so there was no such thing as margins, lines, line height, and spacing. You just wrote stuff where there was space. When this was the way of producing books, there was no preconception of any of this typographic stuff. If you wanted to draw a diagram in the middle of your writing, you just drew a diagram. If you wanted to emphasize a word, you just wrote it bigger. It's gotten more and more difficult to take advantage of these richer axes of expressing yourself. Instead, you have to italicize in one specific way or bold in one specific way. Yes. The richness of writing has calcified into the set of markup that we have today. But it doesn't have to be that way. This is even continuing today. Markdown is so ascendant. I remember when I was in high school, Markdown was not everywhere. Most people were on Google Docs and Word. In Google Docs and Word, it's so easy to make any text any color, any shape, any font you want and put it anywhere. In Markdown, you can't color your text. There's a real cultural shift where the aesthetics of a messy Word Doc or Google Doc are getting sucked out. In some settings that's really nice, but in general, I think that's a loss. I didn't realize that was the way you were going to go. I'm very pro-Markdown. There are a lot of benefits, but it's really interesting. We're circling back to representations. I have a few more high-level ideas, which may lead to where we're going later in the conversation. Hanging over so much of how we think about things today are LLMs. You got at this a little bit with the embeddings note. You say, "Language models decouple the way information is stored from the way information must be presented and consumed by humans. What does that imply about how we interact with information?" I'll turn the question around: what does that imply? A lot of ink has been spilled about the fact that language models are good at translating one form of information to another. I think that's correct, but it's also not the most exciting way that I look at LLMs. It is something that LLMs are great at. Many of the people listening will know that the transformer architecture was originally invented for translation, and that's the primary form of data transformation in the language modeling realm. So this data transformation way of looking at models is fine. The more exciting thing to me, going back to embeddings, is that language models, in the process of being trained, have to internally derive some way of doing something that looks like thinking in a mechanical way. I always love physical metaphors. The most visceral metaphor I have for why language models are cool is this : if I were infinitely wealthy and had a bunch of side projects, one of them would be to build a physical version of GPT-2. Imagine you have a 12-story building. It's like one of those marble-run tracks—a huge Rube Goldberg machine. You put a token at the top, like a ball with the token number 32, and it rolls down. There are all these rails carved into the building, and maybe it takes three days for the ball to roll down. By the time the ball reaches the bottom, it rolls very neatly into the slot for the next token. The reason I think language models are cool is because there is a mechanical process they embody. It's fundamentally geometry happening inside the model that somehow embodies all of this stuff. And while it would be really hard to turn it into a physical building, the intuition is what’s so cool. There is something physical and geometric happening inside the models that is much more physical and geometric than what writing feels like. One kind of data transformation that language models do is from inputs to outputs, which is fine and maybe less surprising because that's what the models are trained to do. A more surprising kind of data transformation they do is internal. A language model uses its first few layers to translate the input tokens into some representation space that is more geometric, which the model can then contend with. It spends most of its compute working with ideas in this domain as shapes. Only the last few layers are used to translate its own kind of internal thought back into human writing. This idea that there's this other way of thinking about stuff that is way more mechanical than writing—that exists and that we can do science on and figure out—is so cool. They're distilling it into essence space and doing work with it in essence space. You've built, written about, and talked about a lot of different examples of ways we might create new representations and new tools, from the Prism project to what a synthesizer for thought would be. A video you shared with me early on was a great talk on liquid art, capturing a lot of these ideas. There's a tool called Loom that you refer to. "Loom's brilliance is that it lets the creation happen in a different perspective : painting in time first, then filling in the details in space. My phrase for this perspective shift is a 'perspective transformation' because it reminds me of coordinate transformations." Synthesizers, you say: "because synthesizers are electronic, unlike traditional instruments, we can attach arbitrary human interfaces to it. This dramatically expands the design space of how humans can interact with music." There's one final one. You were talking about your friend, the founder of Flora. "They told me about a mission statement I always found really inspiring : He wants to allow artists to speak beauty into existence. I love this phrase because all of the focus is on the precision of the words and the ease with which they can conjure ideas into being. There has got to be a way to get there by finding better ways to speak of beauty, rather than by mechanizing the means of beauty production." I love that. I know I said it. It's really interesting. You're dancing around both the mechanical side and this more mystical, ambiguous, almost ineffable side of it. My question is a little lower level. You're describing some cool toys or theories. Synthesizers took a while to be taken seriously by musicians. Where do you think we are with these types of ideas today? What are the seeds that are actually promising beyond fun blog posts? First, I'll comment on finding better ways to speak of beauty rather than mechanizing the means of beauty production. It's kind of mystical, but there's a concrete way to think about it. Python is a better way to speak of software beauty. Mechanizing the means of software production would be writing a language model that's really good at writing C. And I think we're all better for Python existing in the world. Elegance doesn't mean ambiguity. It's more clear. It's more useful abstractions of a certain kind. If you're writing a database, Python is too messy, but if you're writing Instagram, Python may be right. When I think about advancement as a society or civilization, if an alien species landed on Earth tomorrow, what would really impress me? It would be cool if they had a language model. But what would be more cool is if they had really elegant physical theories, or they found a more elegant way to think about music, or they have a thing like the periodic table but even more elegant. These things would be really impressive because they are further compressions of knowledge. Have you read _Story of Your Life_ or seen _Arrival_? Yes. A little bit of that in there. A little bit of that. And these things coexist. Mechanizing the means of working with this knowledge is useful for scaling and automating these things. But that by itself feels kind of empty aesthetically. What are some seeds or places where you're starting to see interesting things that could soon be real in this broad category? This is what I've spent the last two years thinking about. The Prism project is about figuring out ways to read interesting ideas, concepts, or features out of the latent spaces of models. That was really fun. It was also the first time I ever wrote deep learning code, and it was for this research project. A wild, hilarious project. A lot of its outputs are hilarious. It's so funny. You can put in my bio from my blog, turn off the coding feature, turn on the culinary arts feature, and it becomes a bio in the exact same style that says you made 100 dishes. It's super fun and interesting. But the reason it's worth digging into why it feels interesting is that it's a totally different way to interact with ideas. The challenge I've spent the last two years on is how to take this cool toy that lets you play with ideas and make it more of a thing in the world. That's partly about packaging the idea and partly about building a thing that's actually useful—that people want to use, play with, or need for their business. That has turned out to be the difficult part. There are some interesting avenues. A really cool company called Goodfire is applying this in a research domain, using interpretability techniques on non-language modalities like models that understand molecular biology and cell biology. But that's more in the research realm. I'm still very interested in language. The most viable path to production impact I've arrived at is using this to help people make sense of large data sets where nuance is important. An example is understanding people. A big part of venture, the business that Thrive is engaged in, is about understanding people—how they work, where they are, and what drives them. Data sets about people are interesting because there's a lot of structure, but all of the important signal resides outside of that structure. I can get a list of everyone that has ever worked at OpenAI, but that's not what makes them interesting. What makes them interesting is what game they're playing, why they're interested in this company, what they're looking for, and what really excites them. Those things are all between the lines. Perhaps by looking inside the model, we can learn to work with these data sets not just at the level of where people work and what their names are, but more at the level of what is driving this person. Rather than just throwing it to a black box that's going to tell you a bunch of names, you can actually roll it out on a sheet of paper. Different motivations and different intensities of motivations can correspond to different places on this chart. There are so many reasons why that is much more exciting to me. The obvious one is that it's more visual and detailed. Also, working with people and making judgments about people is a really high-stakes thing. It feels important to me, from a principled perspective, that the people making those judgments are exposed to the full complexity of the people this data models. People are a good example because they're a rare category with incredible complexity. Many people have a strong degree of intuitive, high-resolution thinking about it, but they can't always put it into words. There are people with incredibly high emotional intelligence. You see this in all categories, but especially with people : they have this high-dimensional, non-verbal ~vibe or sense or notion that somebody's special. All the words that you're using--are like, what do they even mean? Investor firms will say that a person is special or good or even smart. What's inside all that? They might use the word "smart" in a really specific way. Or people use "spiky." People say "crafty" or "cracked." What are these words? Don't we deserve a better vocabulary for talking about these really important things? Yes, but it's also cool because there is a huge latent space there that is in certain people's heads, and it hasn't been extracted. There is a complexity there for which we currently have very ill-suited tools. I want anyone working with these data models of people and CRMs to contend with the complexity of what's inside people. I think the world would be better if we forced them to, but I also want to give them good tools to be able to do that. One theme that runs across your work, which you alluded to when you were talking about warming up to instrumental interfaces, is this balance between very philosophically inclined, highly principled thinking and being pragmatic and making stuff that's useful. You've done that for the tools you built for yourself. You're doing it professionally, and you might even be doing it ideologically. That very much applies to understanding when an instrumental tool might be useful versus a more engaged one. This is especially relevant when it comes to building with LLMs, which is what you're focused on in both your personal and professional work. You alluded to it earlier : LLMs are a sort of magic box. At least that's what it feels like to a lot of people. You are focused on how we can make these LLM-powered tools more robust and engineered. Sometimes that means making them more instrumental, and sometimes that's with engaged versions. But broadly, you’re moving away from this mystical, ineffable, hallucinatory thing to something that's more reliable and predictable. You say "Natural language interfaces feel like they should be super easy to use, but in practice they can feel confusing and frustrating because they leave no room for affordances that tell you how exactly to command or control the tool. How do we fix this?" Obviously that's--as you said earlier-- specifically referring to chat. At a high level, why are LLMs so different than most of the rest of software tooling and infrastructure, at least when it comes to tactically solving problems? There's a cultural aspect and a technical aspect. The technical aspect maybe comes first. In an academic sense, the thing that's exciting about language models is their generality. But generality is a really undesirable property for an interface to have. You want an interface to be intuitive, and in a lot of cases, very obvious. Generality gives you power, but there's a reason that Final Cut Pro looks very different than an IDE. The thing that makes language models as a technology academically appealing, and what is pushing the frontier of research, is antithetical to the thing that makes really great interfaces for humans to use. Then there is a cultural aspect. This academic desire for generality and simplicity of interface has bled into people who are building products. There is a similar desire for generality without really considering what end users feel like when they're using it, which is : “Okay, you have a box. You were telling me this thing is useful, but I don't know what to ask.” You can do anything with it. You can do anything with it. And real physical objects have less of this problem because they have physical constraints. If you're making a hammer, a hammer needs to have the affecting end—the thing that hits—and then a handle. If you had a box of tools that was just a bunch of squares, that would be a terrible box of tools. But both the academic culture around machine learning and the way that technology originated has influenced this. It's worth talking about both the instrumental side and the engagement side with regard to LLMs, or at least the ends of that spectrum. You say "The interesting details are in the necessary trade-offs between how well you understand the user's intent and how cheaply, quickly, and reliably you can deliver the result." I'm curious how you think about what ideal, instrumentally shaped LLM tools look like today, beyond just an agent. This applies a little bit to what we were talking about before. More practically, if the idealized magic-button, perfect agent is the perfect instrumental LLM tool, what does a good-enough, instrumental-ish LLM tool look like now? Because the technology is so general, as someone building something with language models, it's tempting to want your solution and your problem-framing to also be really general. If you are using a language model to build something to help people write programs, it's really tempting to want to preserve that generality, which is in some senses equivalent to power, and to say you want to build a thing that's just generally useful for anything in programming. It's hard to resist the temptation. LLMs are really good at being general. It has the potential to be. All the elements of goodness are in there for generality, but it's just really hard tactically to make a product that is so general and good at all those general things. The times when I've built products that feel the best are when I've been really specific and precise about what task the model is doing. In the classic ML world—pre-language model, pre-generality pill—the task shape is what you start with. You have a bunch of inputs and a bunch of outputs, and the model is learning a function that maps the inputs to outputs. Because you're building this thing and you can't train over everything in the entire world, you have to be really precise about what the inputs are, what the exception cases are, and what the right behavior is on those exception cases. You have to really master the task domain. You have to have a lot of really fine opinions about and deep knowledge of the complexity in the inputs and outputs. And that principle is still true today. If you want to build good products on top of language models, it really pays off to be an expert in what the right inputs are and what you want the users to type in. You should be opinionated about what the users should type in and what they should want to do with this thing. You should also be opinionated about what the right outputs for those inputs are. You should not just leave it up to the model defaults or whatever OpenAI, Anthropic, or Google's post-trainers want the model to say. That feels like what is missing the most in a lot of AI products today. There's this temptation to try to preserve the generality of the model as much as possible. But there's no true generality. It just means that you are letting the post-trainers of whatever model you're using be your product designers, which I think is not the best. One thing I'm not sure where it sits on the gradient from instrumental to engaged is this broad frame of AI as a collaborator, which we're increasingly contending with. You have a bit where you're suggesting that working with AIs as collaborators or a set of agents might be more like managing people or designing an organization. You say "There may be craft to building software at a distance with things like LLM agents chugging along beneath your hands, but that feels to me like a distinct kind of joy from working directly in the medium of code. You go on to say that "just as we shouldn't expect the same people to enjoy both coding and managing all the time, we shouldn't expect people to enjoy both coding 'by themselves' and coding with a team of agents the same. And I think that's OK. And I think the people like you and me who like the craft of working directly with the system will find ways to accelerate ourselves in our craft." I don't know when you wrote that; I don't think it was super recent. No. I'm curious, have you found yourself doing more of both of these sides? Or are you still more in the "accelerate your craft" zone? I am, definitely. This was a while ago, as you said, and it has come true. One of my friends, Dan Schipper, has a company called Every, and somewhat incredulously, all of the engineers at that company only use coding agents. It's incredibly rare, if at all, that they open up an IDE and write code, which I am somewhat skeptical of, but I believe Dan. Me personally, I really enjoy understanding and having opinions about how to architect systems. I like looking at how things are organized, how things work under the hood, and being in the weeds. I've gotten better and I've still accelerated myself. I'm way faster at building things than I used to be, but in a different way. It's a different kind of joy. As I wrote then, both things will hopefully continue to be accelerated. At the end of the day, software is probably the most complex kind of machine that humans can ever make, just because you can pack so much complexity into such a small amount of space. It's important that as we build more software and more critical infrastructure, humans are forced to contend with the full complexity of the systems we're building. It'll be important that some people—and I think that's good—will continue to get joy out of working with that complexity, including myself. A few bits that I think tie very closely to that and the idea of accelerating craft. You wrote, "When possible, directly manipulating the underlying information or objects of concern—the domain objects—minimizes cognitive load and learning curve." And then a separate quote : "What direct manipulation is to the graphical user interface, we have yet to uncover for this new way to work with information." You go on to talk about good complexity. You say, "A second type of surrogate object is focused not on showing individual attributes, but on revealing intermediate states that otherwise wouldn't have been amenable to direct manipulation because they weren't concrete. Indeed, it's fair to say that direct manipulation is itself merely a means to achieve this more fundamental goal : Let the user easily iterate and explore possibilities, which leads to better decisions." All of this, to me, is around what we talked about with the people and embedding stuff. You call a dataset a "bag of concepts," which is great. This proximity in latent space and another metaphor you use is like a brain. What if we could do a brain scan of an LLM? Yeah. I'm curious what your early conclusions are. Going back to one of those quotes about the GUI, most people today, even engineers, are using it to generate some amount of code—tab, autocomplete, whatever—or as full agents to get answers. The broad shape of the use case for these things doesn't seem to capture this idea of exploring latent space. Why is that? Do you have any hunches on where we're going? Are you doing anything? We've talked around some of this, so I don't want to be too repetitive, but I'm curious where you sense we are on this. I feel this really personally. I've spent so much time thinking about how to work with ideas in this more geometric format, and I'm so excited by it. And yet, even I'm building chatbots. At times it feels like very deep-cutting, personal hypocrisy. At other times, it feels like it's a skill issue and I just haven't been good enough at injecting some of these research ideas into real products. I do think it'll come. Phil Wadler, a researcher who contributed to Haskell, has this great quote about building a great programming language. I'm going to butcher the details, but it's something to the effect of : if you want to build a great programming language, first do some research, invent a language, and then wait 40 years for the ideas to mature. Surely a part of what I need is more patience. There are ways for ideas to collide with the right ingredients for it to become successful over time. It is still what I'm working on. For a while, I was just doing exploration in my head. Then, a few years ago, I was working in a very horizontal company, Notion, and trying to find ways to make useful things with my ideas. Now I'm in a very vertical, concrete set of use cases, still trying to find ways this can plug in and be useful. I think I've gotten closer. The "understanding people" aspect is concrete progress in where I think this can be applied. But I'm even more excited to actually build things in this direction and then see what breaks. Ultimately, that's how ideas get out into the world. That's a great transition. You work at Thrive, if that wasn't obvious. You previously spent some time at Notion, which you just alluded to as being a little bit more wide and abstracted. Thrive works more deeply and is more embedded. Part of that, implicitly, is that Thrive has fewer than 100 people, most of whom you're with in a room or at least in a building. I think you framed this somewhere as a much smaller TAM for the things you're building. Of 80 people, I've spoken to every single one of my potential users. That's amazing. At a high level, what drew you here? And what has that shift in the scope of your work been like? It’s important to know that the engineering we do at Thrive is not super visible. We have a team of five or six of us, including a designer and many great product engineers and data folks. I'm a part of that team for most of my work. The team existed a while before I came, but I came in and kickstarted a lot of the more LLM-related parts of what we're building now. The really interesting thing that I've learned about building internal tools is that when you're building anything, you have a fixed budget for surprise or uncertainty. When you're building a product to have its own life in the world and be used by hundreds of millions of people, you spend a lot of that uncertainty on the problem. Different people are going to bring different problems, use cases, and contexts of use to what you build, and you have to prepare for all of those things. The problem is like a theory—a broader, zoomed-out, abstract thing that has to hold a whole bunch of potential people's needs. Because the uncertainty is in the market and the users, you want to be stable and predictable in what you build. With internal products, at least in my experience at Thrive, the problems are so clear. They're also just barely outside the reach of boring technologies, so there's a lot more room to throw weirder or more frontier ideas against those problems and see what sticks and what works. That's been really fun. You briefly alluded to it, but it would be helpful to hear a little bit more about what you actually work on. Second, you also implied this, but you aren't the typical profile of somebody a VC firm hires to work on internal tooling. That match on both sides is a little unique. A meta question hanging over all of this would be : what does it mean for an investment firm to take building software very seriously? It's funny, I find it really hard to explain what the thing actually is. I had this problem at Notion, and I have this problem again at Thrive. If you work at a calendar app company, you say, "I'm building a calendar." Notion is a little bit of everything. Some people call it a note-taking app, but the thing people say internally is, "It's like Lego bricks for software," which sounds like a pretentious description. Here, there's a different kind of the same problem. The thing we're building is a suite of different tools that together help a bunch of different teams inside Thrive work. It's not really a calendar, a note-taking app, or a chatbot. It's an amalgamation of all of these things to the extent that they're useful for different teams. The mandate of our team is to build a kind of Ironman suit for everyone inside Thrive and to make Thrive's business itself much more software-defined. There are a few different layers to this. One layer is to organize the information and data we have about the world, people, companies, and investors around Thrive to be more accessible. The next layer is a core product that includes a thing that looks like a calendar, a newsfeed, a search product, and a research agent tool. All of these things combine into a single, coherent interface that helps people find information quickly, prepare for their day more efficiently, know what's going on, be more prepared in their job, and spend less time working on drudgery. Then there's a third, more embedded pillar. We have this foundation of data and automations, but there are really specific workflows where if we just pick at that specific workflow and fully or mostly automate it, it's going to save someone tons and tons of time. Maybe only two of our 80 people are going to use it, but it's going to save them so much time that it's worth it. We build those kinds of one-off automations as well. What's cool about building for 80 people is that you can build for every single individual person, so people use it differently. I use it a lot to look up and understand different companies that we're partnered with, involved with, and invested in, and the different people around our universe. Our research agent is also quite good, so I use it sometimes as a replacement for o3 because we have proprietary data. I also use it to organize my day and prep for meetings. So that's what we build —very classic internal products. We've broken down the problem as a data layer, an intelligence layer, a UI on top, and then automations on top, which I think is a really clean separation of concerns. My second question is this : most investment firms, certainly VC firms these days, have internal tools. That's true. I know you personally take building software really seriously. My assumption is that this is, to some degree, to attract someone like you, and that this seriousness is internalized here. The broad question is, what does it mean for an investment firm, in particular, to take building software seriously? Facebook builds really nice internal tools, too, but what does it mean for an investment firm to take building software seriously? There are things that make it personally interesting to me as a place to continue validating some of my weirder ideas about information tools. There are also ways in which this is a cool opportunity for Thrive and for any investment firm. I've already spoken a little bit about how internal tools give you more flexibility in the methods you apply to solve hard problems. The problems are so fixed, and internal tool teams tend to be smaller, more exploratory, and agile. The intellectual work that many people do inside Thrive—the only firm I know with this familiarity—is deeply qualitative. It's not just moving numbers around and optimizing numbers in sheets. You have to understand people, products, problems, research, news, and events. These are all qualitative things. At the same time, there are clear signals for when decisions are correct. In place of raw signals of successful decisions, there are also high-quality I thought it was a good place to continue building high-stakes tools to help people work on deep intellectual and knowledge-bound problems, while being able to flexibly explore inside that. More generally, I've been noodling on the idea that venture is fundamentally a services business—a financial services business. Right now is an interesting time for services businesses because when you're trying to get really good at a service, two things are scarce, especially in an AI world. One is a detailed understanding of what it means to be good at that task—what it means to get good at investing, researching a market, or legal contract review. There's a lot of detail in there that isn't written about on the web. You have to do the job to learn it. The other scarce thing is a sandbox environment, or even a real environment, where you can make decisions and learn through feedback. This is true beyond financial services. If you're running an AI-managed software-building agency, the two scarce things are understanding how to get a client and deliver the software well, and having a playground to write software and get feedback from clients or compilers. It feels like an interesting technical problem, given where AI is now. In both of the examples you gave, you're in a constant state of "scale xor explore." You're in that situation because the next incremental thing might always be new if you are in consulting or investing. In contrast, big software companies have new things, but they are also just really good at mechanical reproduction in some sense, which is interesting. It's an environment that's very rich with really intellectually demanding tasks with a lot of detail. If you built software that either was really good at the task or helped a human get really, really good and efficient at the task, both of those things were really interesting innovations for me to work on. More concretely, Thrive is filled with really thoughtful people. From my experience in the last year, we've been able to build something that's really functional and valuable, but also in a really high-craft way. One of my favorite things about our app is that there's a ribbon of four tabs at the bottom, and when you tap the tab buttons, there's a little haptic vibration. It's just so nice. I love that I can work on things like that—little animations. We've done a lot of work to make our chat interface itself really, really robust. I like that there's space afforded, even if it's only for 80 or 100 people, to make something that is objectively a great piece of software. And that applies to the way that we build our language models as well. Your earlier comment about the Ironman suit made me think of a broader point. One of the more unique things about an investment firm is that investing, in many ways, is the most generalized job. You are definitionally a generalist. As a result, you're building tools for this group of high-agency, fast-learning, fast-moving generalists, which is a broadly interesting design problem. I don't know if I fully agree with that. Especially at a generalist fund, the topics that we want to research and understand are general. One day we'll be looking to understand batteries, and the next day we're looking at fashion. But the skills involved are not super general. I'm not an investor, so maybe I don't have as much detail as someone who would be. But it feels to me like the core skills are around relationship building, things that resemble sales, rapidly understanding and researching new topics, and decision-making under duress. You're describing something that sounds pretty generalist to me. If you really think about what it takes to be good at programming, the skills also include understanding really complex systems and being able to describe that complexity in words really well. The concrete skills in programming are using an IDE; in investing, you use Excel. There are these super-concrete skills, but I feel like they're a little bit too in the weeds. The way that we describe skills, they all can sound general. But I don't think investing is fundamentally more generalizable than many other skills. I do think it's very qualitative and rich in detail. It makes it an interesting task. On that last note, how do you think about bringing an engineering orientation to any kind of work, or this kind of work specifically? One way I've grown as an engineer in the last year is that when I was really young, I viewed my job as writing code that solves the problem. The end artifact, the deliverable, was the source code. I grew a little more mature and wiser, and my perspective changed. My job wasn't just to write the source code, but to deliver a running system. The system is derived from the source code, but it has to be reliable and hit certain operational metrics. It has to be understandable and debuggable. If something goes wrong, you need to be able to root cause it and fix it. If you want to make a change, you need to be confident that it's going to be good. That's the second layer : the operational aspect of the software. More recently, I've been thinking about a third layer, which is especially important when working in a team. The first two layers make sense for a solo developer—writing the source code and then maintaining the software. The third layer you have to deliver is a people system or an organization capable of continuing to make changes to that software and evolve it as the world evolves. In some ways, the thing you have to deliver is not just the software, but the processes, the culture, and the team itself. If you were removed from that equation, the whole system would be self-sustaining, resilient, and robust. It would continue to deliver a resilient, operationally excellent piece of software, as well as really good source code. Yes. Building these three systems—the source code, the running system, and the team—in a way that is easy to change, understandable, and debuggable is what good engineering is, at least as I understand it at this point in my life. More specifically to my job building a tool for Thrive, in the near term, a lot of what we're building is this Ironman suit idea. There are a ton of things that people do inside the firm that we want to make easier and give them more leverage. In the long term, I'm also really excited about whether there are parts of Thrive's business that we can fully turn into a scaling equation, where we spend an incremental dollar of compute and get an incremental, predictable amount of return. It would be really exciting if we had some machine, some black box, where for every X thousand extra dollars you put into compute, you get one new, extra interesting founder that we discover in the world. I don't know how you would do that yet, but that's a fully software-defined version of the business, versus one where you have people. In that world, you would still need the people and the judgment of the people. Yes. It's not about automating. In order to build that thing, you're going to have to have a really deep understanding of people, of the job, and of the decisions. As we want to stay a small team while we grow the business, we'll have to lift everyone up to work at a slightly higher level of abstraction. As we lift those people up, there will perhaps be this underlying engine that's an embodiment of what makes people really good at their current job. This is another cool example of the ways that better modeling latent space might produce really interesting outcomes. In order to build systems that are really good at these very nuanced jobs, you're going to need to give the people that are operating these tools the ability to express their nuance, which goes back to what makes a good tool. Has working in an investment firm, or Thrive specifically, made you more commercial? Or have you just had learnings about that side of the world? I don't think it's a consequence of working at an investment firm specifically. I'm sure these are all related, but I don't think I'm more commercial now than I used to be. Part of it is the growing pains of having an idea that I was really enticed by but then not being able to find great use cases for it. And a part of it also is that Thrive in particular is very generalist, doing everything from fashion to compute. So many of the things that humans are engaged in have nothing to do with software or technology. Everything is technology in some ways, but not necessarily software and AI, or even automation or machines. Software is such a small part of the human experience. There are a lot of other kinds of businesses out there, and that has been a continual learning that I find really fun. I want to spend our last few minutes on an idea that I think has been inside a lot of what we talked about. It's expressed in your final answer and certainly in the tools you build for yourself. You've said "all great tools must be built in a serious context of use, that good tools are transparent, and that they let ideas through." But zooming way out, let's talk about technology and humanity and how they interact. In this section, I have a bunch of quotes for you. The first is from an essay you wrote called "Create Things that Come Alive." You say, "Building technology is fundamentally an affair by humans for other humans, and objects of technology ought to be ensconced in a romance and history and all manners of color and details and textures of life. It ought to come alive in our environment. Technology is not what's shiny and boxy and delivered in metallic wraps." And then you quote Ursula Le Guin : "Technology is the active human interface with the material world." From a separate essay you wrote on Radio City, you say, "I yearn to see this more irreverent and humanist relationship to technology imagined more often today, when both technology and the industry backing its progress feel increasingly detached from culture and media and the humanities. I dream of a humanist revival with computing systems at the center, grasped firmly in the hands of wisdom." And one final set of quotes, where you point to this notion that technology is often thought of as a separate, alien thing from our messy humanity. You say, "Technology exists woven into the physics and politics and romance of the world, and to disentangle it is to suck the life out of it, to sterilize it to the point of exterminating its reason for existence, to condemn it to another piece of junk." "If you consider yourself a technologist, here is your imperative : Build things that are unabashedly, beautifully tangled into all else in life. People and relationships, politics, emotion and pain, understanding, or the lack thereof, being alone, being together, homesickness, adventure, victory, loss. Build things that come alive and drag everything they touch into the realm of the living. And once in a while, if you are so lucky, may you create not just technology, but art. Not only giving us life, but elevating us beyond." Those quotes are amazing. My question is, how do you personally imbue the technology you build with this aliveness, with this humanity? Wow, what a question. Earlier, I mentioned that one way I've grown is I've gone from working on this stuff because I was really excited by productivity tools and their aesthetic, to thinking of it in the context of trying to give people more power and agency over their life and the world. A similar parallel is when I think about why I like working on this stuff. A part of it is that I like the puzzle of writing programs. I feel I'm good at it, and I want to continue doing it and be better at it because mastery itself is fun. But so much of it is also everything about the context in which I do this work. I find the environment of people building companies and Thrive really fun. I love the people I get to work with, get opinions and feedback from, and tell them about the ideas I'm thinking about. All of this exists within the context of being a human being with other human beings. I'm building stuff for them, building with them, or having them disagree in interesting ways. The context of doing this work is what's fun. Over time, I'm sure I'll learn even more and lean even more in this direction. Going back to where we started this conversation, this is really adjacent to another aesthetic I feel : the point of technology is to elevate us, to give us more prosperity, to let individuals do more than just survive. Most of the time, for most people, technology itself is instrumental. There is fun in building technology. It's fun to write programs and solve these puzzles. But on a societal scale, technology itself is an instrumental tool, and it's important not to forget that. If it's instrumental, there has to be a result that we want. What is the prompt? The most inspiring prompt, whose response is technology writ large, is everything that makes it really great, fun, and lovely to be a human being. How can we have more of it? How can everyone have more of it? And how can we have it in all of the variations that exist in the world? That is the prompt. The answer is everything technology has unraveled into. If we stray from that—if we develop some other proxy, some other benchmark that we're optimizing technology towards—that's not good. Yes. People building technology need to remember that this is all for other human beings and about building with other human beings. Another few quotes on a related theme. In a recent tweet, you said, “More people should create things to proliferate an aesthetic into our future, not just to solve problems. This is the quality that every artist and engineer I respect shares most universally. Without this, you are doomed to churning out slop. What values do you create to spread? What image do you dream about? What is the feeling of a tomorrow you want to give form to? Have a position, stand for something. Don't just create value.” In a separate idea about dreams, you said, “Too many tools for thinking, not enough tools for dreaming. But really, aren't some of our best ideas found in dreams?" And finally, you were riffing on neural media : “Could a generative image model, never having seen Voyager 1's pale blue dot, have been used to create such beauty? Can a neural generative model imagine beyond its own world model? How can we build models that can help imagine new worlds, not just permutations of the one we know?” Imagery is the theme across all of those ideas—aesthetics, but maybe even more so, dreams. Dreams are so image-based, at least in my experience. How does imagery underpin your creativity and your work? I realize you're talking about imagery in the literal sense and maybe in this broader sense, too. I do think in a lot of these cases, imagery and dream are interchangeable or at least deeply entangled. Dreams are maybe the more fundamental thing, although imagery is the way in which it comes across. A lot of what I read around me is engaged in pretending that technology has its own arc and its own destiny—that it's going to go somewhere by itself. There is some manifest destiny version of technology where there is some final form that we're trying to achieve. Going back to the idea that humans build technology for other humans, I understand why this is sometimes true. There are powerful structural forces that let certain technology proliferate really easily and have more evolutionary pressure and tailwind. In some sense, it's true. But this is often used as a way for people to either not be intentional about the direction in which they are building the technology or to forget that they have the agency and the responsibility to try to push against it or shape its direction in small ways. It goes back to the first thing you said at the top of the conversation. There is a default path for technology to roll down the hill, and it's still really hard to predict. This fetishization of prediction is most obvious in the question you get in San Francisco all the time : "What is your timeline?" as if there's some correct answer to this question. My pithy response to this always is : What do you mean, "the timeline"? You're the ones that are building it. If you ask me to make a prediction like when I'm going to wake up tomorrow, I understand the spirit behind the question. There's going to be some probabilistic distribution over when I'm likely to wake up. I'm probably more likely to wake up around 8:00 or 9:00 than 5:00 AM. There is a likelihood distribution we're talking about, but this is also a thing where if I really wanted to, I could wake up whenever. We have will. There is some will. The question of when I wake up tomorrow is inane. But in the case of building technology, people should be building it intentionally to express who they want to be and what they want the world to look like. There's a default course, but the fun in technology is to use it to deviate the world from that default course in whatever way you think is interesting, useful, and good for you and for the people around you. So it has a course, but the course is ultimately the sum vector of every person that's pushing on technology in different ways. One of the ways in which I've grown over time is that it used to be the only way I would influence where technology was going was by building stuff. Over time, I've ended up in this really privileged position where a bunch of other people care what I think about stuff. Now I have another lever to push on technology. I can build technology on my own, or I can try to convince lots of other people that there are other directions for technology to go and get their help in pushing things where I want to go, which is what I'm doing now. There are all these different ways that you can tug or nudge technology to go in the direction that you want to see the world look more like. If you're building technology, you have this power and you should take advantage of it. It's incumbent on you to take advantage of it. Aesthetics, dreams, and all these things. Highly rational, ambitious, smart people can underrate their weight, their persuasion, or their impact on what we do. Maybe I'm just romantic, but it's the point of it all. When you're enjoying a delicious meal, there's the functional point, which is to sustain yourself, and then there's the taste. It's totally valid to say, "I'm eating this thing because it tastes amazing." You can make whatever meal, but it's much better to think about trying to make something that tastes really good. The taste of the meal, in my head, is the value and the principles behind what you make. I have one final question, but I have some more quotes before we get there. This is you : "In these visions, I fell in love with the idea that there was some quality other than truthiness that ought to guide our search for knowing more about the universe and about living. This ineffable quality I've come to call by many names. Among them are words like novelty, surprise, and wonder." Also you: "Productivity is about the industrialization of creation. Wonder, in contrast, defies systemization because it gets its power from uncertainty and surprise. By nature, you can't optimize wonder because to optimize requires knowing the output and the process. Wonder is the discovery of new outputs and new ways of getting there." And one final quote from the beginning of a book I love by Lawrence Weschler. It's the biography of Robert Irwin. I'm in the middle of this one right now. Amazing. He opens the book with an anecdote about Irwin: "During the early sixties, when Robert Irwin was on the road a lot visiting art schools and chatting with students, he was proffered an honorary doctorate by the San Francisco Art Institute. The school's graduation ceremony that year took place in an outdoor courtyard on a sunny, breezy afternoon, sparkling clear. Irwin approached the podium and began, 'I wasn't going to accept this degree, except it occurred to me that unless I did, I wasn't going to be able to say that.' He paused, waiting as the mild laughter eddied. 'All I want to say,' he continued, 'is that the wonder is still there.' Whereupon he simply walked away." My final question is: what does it feel like to be lost to wonder? I think at the root of everything that I find really fun and invigorating is a useful, substantive sense of novelty. If I had to clinically analyze it, I think that is what wonder feels like to me. This is part of solving hard programming problems, trying to come up with new abstractions, and spending time with new people. Behind all of those things, there's something new that I feel I could understand if I just put more effort into it and spent more time with it. There's something fundamentally very satisfying about coming upon a thing that feels new and mystical, and then figuring out a model for how to understand it. I feel I'm making the most of my life when I'm in a repeated cycle or a flow of coming upon something new, understanding it, and then coming upon something new and understanding it again. To do that continually, over and over, is to be lost in wonder. That's all I got. Linus, thank you. Thank you very much.
